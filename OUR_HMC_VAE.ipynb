{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, './src')\n",
    "\n",
    "from data import Dataset\n",
    "from kernels import HMC_our, HMC_vanilla, Reverse_kernel\n",
    "from models import Gen_network, Inf_network, Inf_network_simple, Gen_network_simple\n",
    "from target import NN_bernoulli, GMM_target, NN_Gaussian\n",
    "from utils import plot_digit_samples, get_samples\n",
    "from args import get_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchType = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(rand_seed):\n",
    "    torch.cuda.manual_seed_all(rand_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.manual_seed(rand_seed)\n",
    "    np.random.seed(rand_seed)\n",
    "    random.seed(rand_seed)\n",
    "\n",
    "seed = 1 # 1337 #\n",
    "set_seeds(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args()\n",
    "\n",
    "args.data = \"mnist\"\n",
    "args.z_dim = 64\n",
    "args.data_distrib = GMM_target(args, args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.data == 'toy_data':\n",
    "    encoder = Inf_network_simple(kwargs=args).to(args.device)\n",
    "    target = NN_Gaussian(kwargs=args, model=Gen_network_simple(args.z_dim, args), device=args.device).to(args.device)   \n",
    "else:\n",
    "    encoder = Inf_network(kwargs=args).to(args.device)\n",
    "    target = NN_bernoulli(kwargs=args, model=Gen_network(args.z_dim, args), device=args.device).to(args.device)\n",
    "\n",
    "if args.learnable_reverse:\n",
    "    reverse_kernel = Reverse_kernel(kwargs=args).to(args.device)\n",
    "    reverse_params = reverse_kernel.parameters()\n",
    "else:\n",
    "    reverse_params = list([])\n",
    "\n",
    "if args.amortize:\n",
    "    transitions = HMC_our(kwargs=args).to(args.device)\n",
    "else:\n",
    "    transitions = nn.ModuleList([HMC_our(kwargs=args).to(args.device) for _ in range(args['K'])])\n",
    "    \n",
    "if args.fix_transition_params:\n",
    "    for p in transitions.parameters():\n",
    "        transitions.requires_grad_(False)\n",
    "\n",
    "if args.separate_params:\n",
    "    params_decoder = list(target.parameters())\n",
    "    params_inference = list(encoder.parameters()) + list(transitions.parameters()) + list(reverse_params)\n",
    "    optimizer_inference = torch.optim.Adam(params=params_inference, lr=args.learning_rate_inference)\n",
    "    optimizer = torch.optim.Adam(params=params_decoder, lr=args.learning_rate)\n",
    "else:\n",
    "    params = list(encoder.parameters()) + list(target.parameters()) + list(transitions.parameters()) + list(reverse_params)\n",
    "    optimizer = torch.optim.Adam(params=params, lr=args.learning_rate)\n",
    "    \n",
    "optimizer_vanilla = torch.optim.Adam(params=list(encoder.parameters()) + list(target.parameters()), lr= args.learning_rate_vanilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.vanilla_vae_epoches > 0:\n",
    "    batch_size = args.train_batch_size\n",
    "    args.train_batch_size = 100 # just for Vanilla VAE training\n",
    "dataset = Dataset(args, device=args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_code = args.std_normal.sample((64, args.z_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(z_new, p_new, u, p_old, x, sum_log_alpha, sum_log_jac, sum_log_sigma, mu=None, all_directions=None):\n",
    "    if args.learnable_reverse:\n",
    "        log_r = reverse_kernel(z_fin=z_new.detach(), mu=mu.detach(), a=all_directions)\n",
    "        log_m = args.std_normal.log_prob(u).sum(1) + args.std_normal.log_prob(p_old).sum(1) - sum_log_jac - sum_log_sigma + sum_log_alpha\n",
    "    else:\n",
    "        log_r = 0 #-args.K * torch_log_2\n",
    "        log_m = args.std_normal.log_prob(u).sum(1) + args.std_normal.log_prob(p_old).sum(1) - sum_log_jac - sum_log_sigma # + sum_log_alpha\n",
    "        \n",
    "    log_p = target.get_logdensity(z=z_new, x=x) + args.std_normal.log_prob(p_new.sum(1))\n",
    "    elbo_full = log_p + log_r - log_m\n",
    "    grad_elbo = torch.mean(elbo_full + elbo_full.detach() * sum_log_alpha)\n",
    "    return elbo_full, grad_elbo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_vae(args, encoder, target, transitions, dataset):\n",
    "    elbo_list = []\n",
    "    for batch_num, batch_val in enumerate(dataset.next_val_batch()):\n",
    "        if args.learnable_reverse:\n",
    "            all_directions = torch.tensor([], device=args.device)\n",
    "        else:\n",
    "            all_directions = None\n",
    "        mu, sigma = encoder(batch_val)\n",
    "        \n",
    "        sum_log_alpha = torch.zeros(mu.shape[0], dtype=args.torchType, device=args.device) # for grad log alpha accumulation\n",
    "        sum_log_jacobian = torch.zeros(mu.shape[0], dtype=args.torchType, device=args.device) # for log_jacobian accumulation\n",
    "        sum_log_sigma = torch.sum(torch.log(sigma), 1)\n",
    "\n",
    "        u = args.std_normal.sample(mu.shape)\n",
    "        z = mu + sigma * u\n",
    "        \n",
    "        p_old = args.std_normal.sample(z.shape)\n",
    "        cond_vectors = [args.std_normal.sample(p_old.shape) for k in range(args.K)]\n",
    "        p = p_old\n",
    "        \n",
    "        for k in range(args.K):\n",
    "            if args.amortize:\n",
    "                z, p, log_jac, current_log_alphas, directions, _ = transitions.make_transition(q_old=z, x=batch_val,\n",
    "                                                    p_old=p, k=cond_vectors[k], target_distr=target)\n",
    "            else:\n",
    "                z, p, log_jac, current_log_alphas, directions, _ = transitions[k].make_transition(q_old=z, x=batch_val,\n",
    "                                                                    p_old=p, k=cond_vectors[k], target_distr=target) # sample a_i -- directions\n",
    "            if args.learnable_reverse:\n",
    "                all_directions = torch.cat([all_directions, directions.view(-1, 1)], dim=1)\n",
    "            sum_log_alpha = sum_log_alpha + current_log_alphas\n",
    "            sum_log_jacobian = sum_log_jacobian + log_jac  # refresh log jacobian\n",
    "        \n",
    "        elbo_current, _ = compute_loss(z_new=z, p_new=p, u=u, p_old=p_old, x=batch_val, sum_log_alpha=sum_log_alpha,\n",
    "                                    sum_log_jac=sum_log_jacobian, sum_log_sigma=sum_log_sigma, mu=mu, all_directions=all_directions)\n",
    "        \n",
    "        elbo_list.append(elbo_current.cpu().detach().numpy())\n",
    "    mean_val_elbo = np.mean(elbo_list)\n",
    "    return mean_val_elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On batch number 1/590 and on k = 1 we have for  0: 0.4899999890476465 and for +1: 0.5099999886006117\n",
      "> <ipython-input-10-5528be8256ec>(75)<module>()\n",
      "-> target.eval()\n"
     ]
    }
   ],
   "source": [
    "print_info_ = 1\n",
    "torch_log_2 = torch.tensor(np.log(2.), device=args.device, dtype=args.torchType)\n",
    "\n",
    "best_elbo = -float(\"inf\")\n",
    "current_elbo_val = -float(\"inf\")\n",
    "\n",
    "current_tolerance = 0\n",
    "# with torch.autograd.detect_anomaly():\n",
    "for ep in tqdm(range(args.num_epoches)): # cycle over epoches\n",
    "    for b_num, batch_train in enumerate(dataset.next_train_batch()): # cycle over batches\n",
    "        target.decoder.train()\n",
    "        plt.close()        \n",
    "\n",
    "        mu, sigma = encoder(batch_train) # sample mu and sigma from encoder\n",
    "        u = args.std_normal.sample(mu.shape) # sample random tensor for reparametrization trick\n",
    "        z = mu + sigma * u # reperametrization trick\n",
    "        sum_log_sigma = torch.sum(torch.log(sigma), 1)\n",
    "        if ep < args.vanilla_vae_epoches:\n",
    "            log_p = target.get_logdensity(z=z, x=batch_train)\n",
    "            log_m = args.std_normal.log_prob(u).sum(1) - sum_log_sigma\n",
    "            elbo_full = torch.mean(log_p - log_m)\n",
    "            (-elbo_full).backward()\n",
    "            optimizer_vanilla.step()\n",
    "            optimizer_vanilla.zero_grad()\n",
    "            if b_num == (args.train_batch_size - 1):\n",
    "                args.train_batch_size = batch_size\n",
    "                dataset = Dataset(args, device=args.device)\n",
    "        else:\n",
    "            target.train()\n",
    "            p_old = args.std_normal.sample(z.shape)\n",
    "            cond_vectors = [args.std_normal.sample(p_old.shape) for k in range(args.K)]\n",
    "\n",
    "            sum_log_alpha = torch.zeros(mu.shape[0], dtype=args.torchType, device=args.device) # for grad log alpha accumulation\n",
    "            sum_log_jacobian = torch.zeros(mu.shape[0], dtype=args.torchType, device=args.device) # for log_jacobian accumulation\n",
    "            p = p_old\n",
    "            if args.learnable_reverse:\n",
    "                all_directions = torch.tensor([], device=args.device)\n",
    "            else:\n",
    "                all_directions = None\n",
    "            for k in range(args.K):\n",
    "                # sample alpha - transition probabilities \n",
    "                if args.amortize:\n",
    "#                     pdb.set_trace()\n",
    "                    z, p, log_jac, current_log_alphas, directions, _ = transitions.make_transition(q_old=z, x=batch_train,\n",
    "                                                        p_old=p, k=cond_vectors[k], target_distr=target)\n",
    "                else:\n",
    "                    z, p, log_jac, current_log_alphas, directions, _ = transitions[k].make_transition(q_old=z, x=batch_train,\n",
    "                                                                        p_old=p, k=cond_vectors[k], target_distr=target) # sample a_i -- directions\n",
    "                if ep  % print_info_ == 0 and b_num % (100 * print_info_) == 0:\n",
    "                    print('On batch number {}/{} and on k = {} we have for  0: {} and for +1: {}'.format(b_num + 1,\n",
    "                                                                            dataset.train.shape[0] // args['train_batch_size'],\n",
    "                                                                               k + 1,\n",
    "                                                        (directions==0.).to(float).mean(),\n",
    "                                                                        (directions==1.).to(float).mean()))\n",
    "                    if args.amortize:\n",
    "                        print('Stepsize {}'.format(np.exp(transitions.gamma.cpu().detach().item())))\n",
    "                        print('Autoregression coeff {}'.format(torch.sigmoid(transitions.alpha_logit).cpu().detach().item()))\n",
    "                if args.learnable_reverse:\n",
    "                    all_directions = torch.cat([all_directions, directions.view(-1, 1)], dim=1)\n",
    "                # Accumulate alphas\n",
    "                sum_log_alpha = sum_log_alpha + current_log_alphas\n",
    "                sum_log_jacobian = sum_log_jacobian + log_jac  # refresh log jacobian\n",
    "            ##############################################\n",
    "            if args.hoffman_idea:\n",
    "                if args.learnable_reverse:\n",
    "                    log_r = reverse_kernel(z_fin=z.detach(), mu=mu.detach(), a=all_directions)\n",
    "                    log_m = args.std_normal.log_prob(u).sum(1) + args.std_normal.log_prob(p_old).sum(1) - sum_log_jacobian - sum_log_sigma + sum_log_alpha\n",
    "                else:\n",
    "                    log_r = 0 #-args.K * torch_log_2\n",
    "                    log_m = args.std_normal.log_prob(u).sum(1) + args.std_normal.log_prob(p_old).sum(1) - sum_log_jacobian - sum_log_sigma # + sum_log_alpha\n",
    "                log_p = target.get_logdensity(z=z, x=batch_train) + args.std_normal.log_prob(p.sum(1))\n",
    "                elbo_full = log_p + log_r - log_m\n",
    "#                 pdb.set_trace()\n",
    "                ### Gradient of the first objective:\n",
    "                target.eval()\n",
    "                obj_1 = torch.mean(elbo_full + elbo_full.detach() * sum_log_alpha)\n",
    "                (-obj_1).backward(retain_graph=True)\n",
    "                optimizer_inference.step()\n",
    "                optimizer_inference.zero_grad()\n",
    "                optimizer.zero_grad() \n",
    "                \n",
    "                ### Gradient of the second objective:\n",
    "                target.train()\n",
    "                log_p = target.get_logdensity(z=z.detach(), x=batch_train) + args.std_normal.log_prob(p.detach()).sum(1)\n",
    "                elbo_full = log_p + log_r - log_m\n",
    "                obj_2 = torch.mean(elbo_full + elbo_full.detach() * sum_log_alpha)\n",
    "                (-obj_2).backward()\n",
    "                optimizer.step()\n",
    "                optimizer_inference.zero_grad()\n",
    "                optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "            else:\n",
    "                elbo_full, grad_elbo = compute_loss(z_new=z, p_new=p, u=u, p_old=p_old, x=batch_train, sum_log_alpha=sum_log_alpha,\n",
    "                                                    sum_log_jac=sum_log_jacobian, sum_log_sigma=sum_log_sigma, mu=mu, all_directions=all_directions)\n",
    "                (-grad_elbo).backward()\n",
    "            if args.clip_norm:\n",
    "                torch.nn.utils.clip_grad_norm_(itertools.chain(*params), args.clip_value)\n",
    "                \n",
    "            if args.separate_params: # if we separate params of inference part and generation part\n",
    "                optimizer_inference.step() # we always perform step for inference part\n",
    "                if (ep - args.vanilla_vae_epoches) % args.train_only_inference_period > args.train_only_inference_cutoff: # but sometimes for gen\n",
    "                    optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                optimizer_inference.zero_grad()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "    ###### validation ######\n",
    "    target.decoder.eval()\n",
    "#             pdb.set_trace()\n",
    "    current_elbo_val = validate_vae(args=args, encoder=encoder, target=target, transitions=transitions, dataset=dataset)\n",
    "    if (current_elbo_val != current_elbo_val).sum():\n",
    "        print('NAN appeared!')\n",
    "        raise ValueError\n",
    "    if current_elbo_val > best_elbo:\n",
    "        current_tolerance = 0\n",
    "        best_elbo = current_elbo_val\n",
    "        if not os.path.exists('./models/{}/'.format(args.data)):\n",
    "            os.makedirs('./models/{}/'.format(args.data))\n",
    "        torch.save(encoder,\n",
    "            './models/{}/best_encoder_data_{}_K_{}_N_{}_fixtransition_{}_amortize_{}_learnreverse_{}_vanillaepoches_{}.pt'.format(args.data,\n",
    "                                    args.data, args.K, args.N, args.fix_transition_params, args.amortize, args.learnable_reverse, args.vanilla_vae_epoches))\n",
    "        torch.save(target.decoder,\n",
    "            './models/{}/best_decoder_data_{}_K_{}_N_{}_fixtransition_{}_amortize_{}_learnreverse_{}_vanillaepoches_{}.pt'.format(args.data,\n",
    "                                    args.data, args.K, args.N, args.fix_transition_params, args.amortize, args.learnable_reverse, args.vanilla_vae_epoches))\n",
    "        torch.save(transitions,\n",
    "            './models/{}/best_transitions_data_{}_K_{}_N_{}_fixtransition_{}_amortize_{}_learnreverse_{}_vanillaepoches_{}.pt'.format(args.data,\n",
    "                                    args.data, args.K, args.N, args.fix_transition_params, args.amortize, args.learnable_reverse, args.vanilla_vae_epoches))\n",
    "        if args.learnable_reverse:\n",
    "            torch.save(reverse_kernel,\n",
    "            './models/{}/best_reverse_data_{}_K_{}_N_{}_fixtransition_{}_amortize_{}_learnreverse_{}_vanillaepoches_{}.pt'.format(args.data,\n",
    "                                    args.data, args.K, args.N, args.fix_transition_params, args.amortize, args.learnable_reverse, args.vanilla_vae_epoches))\n",
    "\n",
    "    else:\n",
    "        current_tolerance += 1\n",
    "        if current_tolerance >= args.early_stopping_tolerance:\n",
    "            print(\"Early stopping on epoch {} (effectively trained for {} epoches)\".format(ep,\n",
    "                                              ep - args.early_stopping_tolerance))\n",
    "            break\n",
    "                \n",
    "    if ep % print_info_ == 0:\n",
    "        target.decoder.eval()\n",
    "        print('Current epoch:', (ep + 1), '\\t', 'Current ELBO train:', elbo_full.detach().mean().item())\n",
    "        print('Best elbo validation', best_elbo)\n",
    "        print('Current elbo validation', current_elbo_val)\n",
    "        plot_digit_samples(samples=get_samples(target.decoder, random_code), args=args, epoch=ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_elbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo_full.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(args.learnable_reverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(args, device=args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.data == 'toy_data':\n",
    "    encoder = Inf_network_simple(kwargs=args).to(args.device)\n",
    "    target = NN_Gaussian(kwargs=args, model=Gen_network_simple(args.z_dim, args), device=args.device).to(args.device)   \n",
    "else:\n",
    "    encoder = Inf_network(kwargs=args).to(args.device)\n",
    "    target = NN_bernoulli(kwargs=args, model=Gen_network(args.z_dim, args), device=args.device).to(args.device)\n",
    "\n",
    "params = [encoder.parameters(), target.parameters()]\n",
    "optimizer = torch.optim.Adam(params=itertools.chain(*params), lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_info_ = 10\n",
    "\n",
    "# with torch.autograd.detect_anomaly():\n",
    "for ep in tqdm(range(args.num_epoches)): # cycle over epoches\n",
    "    for b_num, batch_train in enumerate(dataset.next_train_batch()): # cycle over batches\n",
    "        plt.close()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        mu, sigma = encoder(batch_train) # sample mu and sigma from encoder\n",
    "        u = args.std_normal.sample(mu.shape) # sample random tensor for reparametrization trick\n",
    "        z = mu + sigma * u # reperametrization trick\n",
    "        \n",
    "        mu_dec, sigma_dec = target.decoder(z)\n",
    "        if args.data == 'toy_data':\n",
    "            log_numenator = torch.distributions.Normal(loc=mu_dec, scale=sigma_dec).log_prob(batch_train).sum(1) + args.std_normal.log_prob(z).sum(1)\n",
    "        else:\n",
    "            log_numenator = torch.distributions.Bernoulli(logits=mu_dec).log_prob(batch_train).sum([1, 2, 3]) + args.std_normal.log_prob(z).sum(1)\n",
    "        log_denumenator = -torch.sum(torch.log(sigma), 1) + args.std_normal.log_prob(u).sum(1)\n",
    "        elbo = torch.mean(log_numenator - log_denumenator)\n",
    "        (-elbo).backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    if ep % print_info_ == 0:\n",
    "        if args.data == 'toy_data':\n",
    "            print('Current epoch:', (ep + 1), '\\t', 'Current ELBO:', elbo.detach().mean().item())\n",
    "            print('Mean abs mu0:', torch.mean(torch.abs(mu_dec[:, 0])).cpu().detach().numpy())\n",
    "            print('Mean sigma', torch.mean(sigma).cpu().detach().numpy())\n",
    "            print('Max sigma', torch.max(sigma).cpu().detach().numpy())\n",
    "            print('Min sigma', torch.min(sigma).cpu().detach().numpy())\n",
    "            plt.scatter(batch_train.cpu().detach().numpy()[:, 0], batch_train.cpu().detach().numpy()[:, 1], label='Data')\n",
    "            plt.scatter(mu_dec.cpu().detach().numpy()[:, 0], mu_dec.cpu().detach().numpy()[:, 1], label='Reconstructed')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        else:\n",
    "            plot_digit_samples(samples=get_samples(target.decoder, random_code), args=args, epoch=ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.decoder.linear1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Condatorch",
   "language": "python",
   "name": "condatorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
