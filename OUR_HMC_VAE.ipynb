{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, './src')\n",
    "\n",
    "from data import Dataset\n",
    "from kernels import HMC_our, HMC_vanilla, Reverse_kernel\n",
    "from models import Gen_network, Inf_network, Inf_network_simple, Gen_network_simple\n",
    "from target import NN_bernoulli, GMM_target, NN_Gaussian\n",
    "from utils import plot_digit_samples, get_samples\n",
    "from args import get_args\n",
    "\n",
    "from pyro.nn import AutoRegressiveNN, DenseNN\n",
    "from pyro.distributions.transforms import NeuralAutoregressive, AffineAutoregressive, AffineCoupling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchType = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(rand_seed):\n",
    "    torch.cuda.manual_seed_all(rand_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.manual_seed(rand_seed)\n",
    "    np.random.seed(rand_seed)\n",
    "    random.seed(rand_seed)\n",
    "\n",
    "seed = 1 # 1337 #\n",
    "set_seeds(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args()\n",
    "\n",
    "args.data = \"mnist\"\n",
    "args.z_dim = 64\n",
    "args.data_distrib = GMM_target(args, args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.data == 'toy_data':\n",
    "    encoder = Inf_network_simple(kwargs=args).to(args.device)\n",
    "    target = NN_Gaussian(kwargs=args, model=Gen_network_simple(args.z_dim, args), device=args.device).to(args.device)   \n",
    "else:\n",
    "    encoder = Inf_network(kwargs=args).to(args.device)\n",
    "    target = NN_bernoulli(kwargs=args, model=Gen_network(args.z_dim, args), device=args.device).to(args.device)\n",
    "\n",
    "if args.learnable_reverse:\n",
    "    reverse_kernel = Reverse_kernel(kwargs=args).to(args.device)\n",
    "    reverse_params = reverse_kernel.parameters()\n",
    "else:\n",
    "    reverse_params = list([])\n",
    "    \n",
    "    \n",
    "prior_params = list([])\n",
    "prior_flow = None\n",
    "\n",
    "if args.nf_prior:\n",
    "    flows = []\n",
    "    for i in range(args.num_flows_prior):\n",
    "        if args.nf_prior == 'IAF':\n",
    "            one_arn = AutoRegressiveNN(args.z_dim, [2 * args.z_dim]).to(args.device)\n",
    "            one_flow = AffineAutoregressive(one_arn)\n",
    "        elif args.nf_prior == 'RNVP':\n",
    "            hypernet = DenseNN(input_dim=args.z_dim // 2, hidden_dims=[2 * args.z_dim, 2 * args.z_dim],\n",
    "                    param_dims=[args.z_dim - args.z_dim // 2, args.z_dim - args.z_dim // 2]).to(args.device)\n",
    "            one_flow = AffineCoupling(args.z_dim // 2, hypernet).to(args.device)\n",
    "        flows.append(one_flow)\n",
    "    prior_flow = nn.ModuleList(flows)\n",
    "    prior_params = list(prior_flow.parameters())\n",
    "\n",
    "if args.amortize:\n",
    "    transitions = HMC_our(kwargs=args).to(args.device)\n",
    "else:\n",
    "    transitions = nn.ModuleList([HMC_our(kwargs=args).to(args.device) for _ in range(args['K'])])\n",
    "    \n",
    "if args.fix_transition_params:\n",
    "    for p in transitions.parameters():\n",
    "        transitions.requires_grad_(False)\n",
    "\n",
    "if args.separate_params:\n",
    "    params_decoder = list(target.parameters())\n",
    "    params_inference = list(encoder.parameters()) + list(transitions.parameters()) + list(reverse_params) + prior_params\n",
    "    optimizer_inference = torch.optim.Adam(params=params_inference, lr=args.learning_rate_inference)\n",
    "    optimizer = torch.optim.Adam(params=params_decoder, lr=args.learning_rate)\n",
    "else:\n",
    "    params = list(encoder.parameters()) + list(target.parameters()) + list(transitions.parameters()) + list(reverse_params) + prior_params\n",
    "    optimizer = torch.optim.Adam(params=params, lr=args.learning_rate)\n",
    "    \n",
    "optimizer_vanilla = torch.optim.Adam(params=list(encoder.parameters()) + list(target.parameters()) + prior_params, lr=args.learning_rate_vanilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.vanilla_vae_epoches > 0:\n",
    "    batch_size = args.train_batch_size\n",
    "    args.train_batch_size = 100 # just for Vanilla VAE training\n",
    "dataset = Dataset(args, device=args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_code = args.std_normal.sample((64, args.z_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prior(args, inf_samples, prior_flow):\n",
    "    if args.nf_prior:\n",
    "        # Note, that here I am using T^+1 as T^-1\n",
    "        log_jac_flow = 0.\n",
    "        prev_v = inf_samples\n",
    "        for flow_num in range(args.num_flows_prior):\n",
    "            u = prior_flow[flow_num](prev_v)\n",
    "            log_jac_flow += prior_flow[flow_num].log_abs_det_jacobian(prev_v, u)\n",
    "            prev_v = u\n",
    "        prior = -1. / 2 * torch.sum(u * u, 1) + log_jac_flow\n",
    "    else:\n",
    "        prior = -1. / 2 * torch.sum(inf_samples * inf_samples, 1)\n",
    "    return prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(z_new, p_new, u, p_old, x, sum_log_alpha, sum_log_jac, sum_log_sigma, mu, all_directions, get_prior, args, prior_flow):\n",
    "    if args.learnable_reverse:\n",
    "        log_r = reverse_kernel(z_fin=z_new.detach(), mu=mu.detach(), a=all_directions)\n",
    "        log_m = args.std_normal.log_prob(u).sum(1) + args.std_normal.log_prob(p_old).sum(1) - sum_log_jac - sum_log_sigma + sum_log_alpha\n",
    "    else:\n",
    "        log_r = 0 #-args.K * torch_log_2\n",
    "        log_m = args.std_normal.log_prob(u).sum(1) + args.std_normal.log_prob(p_old).sum(1) - sum_log_jac - sum_log_sigma # + sum_log_alpha\n",
    "        \n",
    "    log_p = target.get_logdensity(z=z_new, x=x, prior=get_prior, args=args, prior_flow=prior_flow) + args.std_normal.log_prob(p_new.sum(1))\n",
    "    elbo_full = log_p + log_r - log_m\n",
    "    grad_elbo = torch.mean(elbo_full + elbo_full.detach() * sum_log_alpha)\n",
    "    return elbo_full, grad_elbo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_vae(args, encoder, target, transitions, dataset, get_prior, prior_flow):\n",
    "    elbo_list = []\n",
    "    for batch_num, batch_val in enumerate(dataset.next_val_batch()):\n",
    "        if args.learnable_reverse:\n",
    "            all_directions = torch.tensor([], device=args.device)\n",
    "        else:\n",
    "            all_directions = None\n",
    "        mu, sigma = encoder(batch_val)\n",
    "        \n",
    "        sum_log_alpha = torch.zeros(mu.shape[0], dtype=args.torchType, device=args.device) # for grad log alpha accumulation\n",
    "        sum_log_jacobian = torch.zeros(mu.shape[0], dtype=args.torchType, device=args.device) # for log_jacobian accumulation\n",
    "        sum_log_sigma = torch.sum(torch.log(sigma), 1)\n",
    "\n",
    "        u = args.std_normal.sample(mu.shape)\n",
    "        z = mu + sigma * u\n",
    "        \n",
    "        p_old = args.std_normal.sample(z.shape)\n",
    "        cond_vectors = [args.std_normal.sample(p_old.shape) for k in range(args.K)]\n",
    "        p = p_old\n",
    "        \n",
    "        for k in range(args.K):\n",
    "            if args.amortize:\n",
    "                z, p, log_jac, current_log_alphas, directions, _ = transitions.make_transition(q_old=z, x=batch_val,\n",
    "                                                    p_old=p, k=cond_vectors[k], target_distr=target, args=args, get_prior=get_prior, prior_flow=prior_flow)\n",
    "            else:\n",
    "                z, p, log_jac, current_log_alphas, directions, _ = transitions[k].make_transition(q_old=z, x=batch_val,\n",
    "                                                                    p_old=p, k=cond_vectors[k], target_distr=target, args=args, get_prior=get_prior, prior_flow=prior_flow) # sample a_i -- directions\n",
    "            if args.learnable_reverse:\n",
    "                all_directions = torch.cat([all_directions, directions.view(-1, 1)], dim=1)\n",
    "            sum_log_alpha = sum_log_alpha + current_log_alphas\n",
    "            sum_log_jacobian = sum_log_jacobian + log_jac  # refresh log jacobian\n",
    "        \n",
    "        elbo_current, _ = compute_loss(z_new=z, p_new=p, u=u, p_old=p_old, x=batch_val, sum_log_alpha=sum_log_alpha,\n",
    "                                    sum_log_jac=sum_log_jacobian, sum_log_sigma=sum_log_sigma, mu=mu, all_directions=all_directions,\n",
    "                                      get_prior=get_prior, args=args, prior_flow=prior_flow)\n",
    "        \n",
    "        elbo_list.append(elbo_current.cpu().detach().numpy())\n",
    "    mean_val_elbo = np.mean(elbo_list)\n",
    "    return mean_val_elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On batch number 1/590 and on k = 1 we have for  0: 0.47999998927116394 and for +1: 0.5199999883770943\n",
      "On batch number 1/590 and on k = 2 we have for  0: 0.5699999872595072 and for +1: 0.42999999038875103\n",
      "On batch number 101/590 and on k = 1 we have for  0: 0.47999998927116394 and for +1: 0.5199999883770943\n",
      "On batch number 101/590 and on k = 2 we have for  0: 0.4399999901652336 and for +1: 0.5599999874830246\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1df7214f106b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     z, p, log_jac, current_log_alphas, directions, _ = transitions[k].make_transition(q_old=z, x=batch_train,\n\u001b[0;32m---> 48\u001b[0;31m                                                                         p_old=p, k=cond_vectors[k], target_distr=target, args=args, get_prior=get_prior, prior_flow=prior_flow) # sample a_i -- directions\n\u001b[0m\u001b[1;32m     49\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mep\u001b[0m  \u001b[0;34m%\u001b[0m \u001b[0mprint_info_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb_num\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprint_info_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                     print('On batch number {}/{} and on k = {} we have for  0: {} and for +1: {}'.format(b_num + 1,\n",
      "\u001b[0;32m~/github/hmc_ouralg/kernels.py\u001b[0m in \u001b[0;36mmake_transition\u001b[0;34m(self, q_old, p_old, target_distr, k, x, flows, args, get_prior, prior_flow)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mlog_jac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_old\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_old\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m############ Then we compute new points and densities ############\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mq_upd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_upd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_old\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq_old\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_old\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_distr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mtarget_log_density_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_distr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logdensity_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq_upd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_prior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_flow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprior_flow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd_normal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_upd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/hmc_ouralg/kernels.py\u001b[0m in \u001b[0;36m_forward_step\u001b[0;34m(self, q_old, x, k, target, p_old)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mq_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mp_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# NOTE that we are using log-density, not energy!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mp_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# NOTE that we are using log-density, not energy!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mq_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/hmc_ouralg/kernels.py\u001b[0m in \u001b[0;36mget_grad\u001b[0;34m(self, q, target, x)\u001b[0m\n\u001b[1;32m    140\u001b[0m                 0]\n\u001b[1;32m    141\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             grad = torch.autograd.grad(target.get_logdensity(x=x, z=q_init).sum(), q_init)[\n\u001b[0m\u001b[1;32m    143\u001b[0m         0]\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/hmc_ouralg/src/target.py\u001b[0m in \u001b[0;36mget_logdensity\u001b[0;34m(self, x, z)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mlog_density\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlog\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \"\"\"\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mp_x_given_z_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0mp_x_given_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBernoulli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp_x_given_z_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mexpected_log_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_x_given_z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/condatorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/hmc_ouralg/src/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh2_flatten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mh3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftplus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mh4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftplus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mbernoulli_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbernoulli_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/condatorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/condatorch/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    776\u001b[0m         return F.conv_transpose2d(\n\u001b[1;32m    777\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             output_padding, self.groups, self.dilation)\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print_info_ = 1\n",
    "torch_log_2 = torch.tensor(np.log(2.), device=args.device, dtype=args.torchType)\n",
    "\n",
    "best_elbo = -float(\"inf\")\n",
    "current_elbo_val = -float(\"inf\")\n",
    "\n",
    "current_tolerance = 0\n",
    "# with torch.autograd.detect_anomaly():\n",
    "for ep in tqdm(range(args.num_epoches)): # cycle over epoches\n",
    "    for b_num, batch_train in enumerate(dataset.next_train_batch()): # cycle over batches\n",
    "        target.decoder.train()\n",
    "        plt.close()        \n",
    "\n",
    "        mu, sigma = encoder(batch_train) # sample mu and sigma from encoder\n",
    "        u = args.std_normal.sample(mu.shape) # sample random tensor for reparametrization trick\n",
    "        z = mu + sigma * u # reperametrization trick\n",
    "        sum_log_sigma = torch.sum(torch.log(sigma), 1)\n",
    "        if ep < args.vanilla_vae_epoches:\n",
    "            log_p = target.get_logdensity(z=z, x=batch_train)\n",
    "            log_m = args.std_normal.log_prob(u).sum(1) - sum_log_sigma\n",
    "            elbo_full = torch.mean(log_p - log_m)\n",
    "            (-elbo_full).backward()\n",
    "            optimizer_vanilla.step()\n",
    "            optimizer_vanilla.zero_grad()\n",
    "            if b_num == (args.train_batch_size - 1):\n",
    "                args.train_batch_size = batch_size\n",
    "                dataset = Dataset(args, device=args.device)\n",
    "        else:\n",
    "            target.train()\n",
    "            p_old = args.std_normal.sample(z.shape)\n",
    "            cond_vectors = [args.std_normal.sample(p_old.shape) for k in range(args.K)]\n",
    "\n",
    "            sum_log_alpha = torch.zeros(mu.shape[0], dtype=args.torchType, device=args.device) # for grad log alpha accumulation\n",
    "            sum_log_jacobian = torch.zeros(mu.shape[0], dtype=args.torchType, device=args.device) # for log_jacobian accumulation\n",
    "            p = p_old\n",
    "            if args.learnable_reverse:\n",
    "                all_directions = torch.tensor([], device=args.device)\n",
    "            else:\n",
    "                all_directions = None\n",
    "            for k in range(args.K):\n",
    "                # sample alpha - transition probabilities \n",
    "                if args.amortize:\n",
    "#                     pdb.set_trace()\n",
    "                    z, p, log_jac, current_log_alphas, directions, _ = transitions.make_transition(q_old=z, x=batch_train,\n",
    "                                                        p_old=p, k=cond_vectors[k], target_distr=target, args=args, get_prior=get_prior, prior_flow=prior_flow)\n",
    "                else:\n",
    "                    z, p, log_jac, current_log_alphas, directions, _ = transitions[k].make_transition(q_old=z, x=batch_train,\n",
    "                                                                        p_old=p, k=cond_vectors[k], target_distr=target, args=args, get_prior=get_prior, prior_flow=prior_flow) # sample a_i -- directions\n",
    "                if ep  % print_info_ == 0 and b_num % (100 * print_info_) == 0:\n",
    "                    print('On batch number {}/{} and on k = {} we have for  0: {} and for +1: {}'.format(b_num + 1,\n",
    "                                                                            dataset.train.shape[0] // args['train_batch_size'],\n",
    "                                                                               k + 1,\n",
    "                                                        (directions==0.).to(float).mean(),\n",
    "                                                                        (directions==1.).to(float).mean()))\n",
    "                    if args.amortize:\n",
    "                        print('Stepsize {}'.format(np.exp(transitions.gamma.cpu().detach().item())))\n",
    "                        print('Autoregression coeff {}'.format(torch.sigmoid(transitions.alpha_logit).cpu().detach().item()))\n",
    "                if args.learnable_reverse:\n",
    "                    all_directions = torch.cat([all_directions, directions.view(-1, 1)], dim=1)\n",
    "                # Accumulate alphas\n",
    "                sum_log_alpha = sum_log_alpha + current_log_alphas\n",
    "                sum_log_jacobian = sum_log_jacobian + log_jac  # refresh log jacobian\n",
    "            ##############################################\n",
    "            if args.hoffman_idea:\n",
    "                if args.learnable_reverse:\n",
    "                    log_r = reverse_kernel(z_fin=z.detach(), mu=mu.detach(), a=all_directions)\n",
    "                    log_m = args.std_normal.log_prob(u).sum(1) + args.std_normal.log_prob(p_old).sum(1) - sum_log_jacobian - sum_log_sigma + sum_log_alpha\n",
    "                else:\n",
    "                    log_r = 0 #-args.K * torch_log_2\n",
    "                    log_m = args.std_normal.log_prob(u).sum(1) + args.std_normal.log_prob(p_old).sum(1) - sum_log_jacobian - sum_log_sigma # + sum_log_alpha\n",
    "                log_p = target.get_logdensity(z=z, x=batch_train, prior=get_prior, args=args, prior_flow=prior_flow) + args.std_normal.log_prob(p.sum(1))\n",
    "                elbo_full = log_p + log_r - log_m\n",
    "#                 pdb.set_trace()\n",
    "                ### Gradient of the first objective:\n",
    "                target.eval()\n",
    "                obj_1 = torch.mean(elbo_full + elbo_full.detach() * sum_log_alpha)\n",
    "                (-obj_1).backward(retain_graph=True)\n",
    "                optimizer_inference.step()\n",
    "                optimizer_inference.zero_grad()\n",
    "                optimizer.zero_grad() \n",
    "                \n",
    "                ### Gradient of the second objective:\n",
    "                target.train()\n",
    "                log_p = target.get_logdensity(z=z.detach(), x=batch_train, prior=get_prior, args=args, prior_flow=prior_flow) + args.std_normal.log_prob(p.detach()).sum(1)\n",
    "                elbo_full = log_p - log_m\n",
    "                obj_2 = torch.mean(elbo_full + elbo_full.detach() * sum_log_alpha)\n",
    "                (-obj_2).backward()\n",
    "                optimizer.step()\n",
    "                optimizer_inference.zero_grad()\n",
    "                optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "            else:\n",
    "                elbo_full, grad_elbo = compute_loss(z_new=z, p_new=p, u=u, p_old=p_old, x=batch_train, sum_log_alpha=sum_log_alpha,\n",
    "                                                    sum_log_jac=sum_log_jacobian, sum_log_sigma=sum_log_sigma, mu=mu,\n",
    "                                                    all_directions=all_directions, get_prior=get_prior, args=args, prior_flow=prior_flow)\n",
    "                (-grad_elbo).backward()\n",
    "            if args.clip_norm:\n",
    "                torch.nn.utils.clip_grad_norm_(itertools.chain(*params), args.clip_value)\n",
    "                \n",
    "            if args.separate_params: # if we separate params of inference part and generation part\n",
    "                optimizer_inference.step() # we always perform step for inference part\n",
    "                if (ep - args.vanilla_vae_epoches) % args.train_only_inference_period > args.train_only_inference_cutoff: # but sometimes for gen\n",
    "                    optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                optimizer_inference.zero_grad()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "    ###### validation ######\n",
    "    target.decoder.eval()\n",
    "#             pdb.set_trace()\n",
    "    current_elbo_val = validate_vae(args=args, encoder=encoder, target=target, transitions=transitions, dataset=dataset)\n",
    "    if (current_elbo_val != current_elbo_val).sum():\n",
    "        print('NAN appeared!')\n",
    "        raise ValueError\n",
    "    if current_elbo_val > best_elbo:\n",
    "        current_tolerance = 0\n",
    "        best_elbo = current_elbo_val\n",
    "        if not os.path.exists('./models/{}/'.format(args.data)):\n",
    "            os.makedirs('./models/{}/'.format(args.data))\n",
    "        torch.save(encoder,\n",
    "            './models/{}/best_encoder_data_{}_K_{}_N_{}_fixtransition_{}_amortize_{}_learnreverse_{}_vanillaepoches_{}_hoffmanlike_{}_separateparams_{}_usebatchnorm_{}_decoder_{}.pt'.format(args.data,\n",
    "                                    args.data, args.K, args.N, args.fix_transition_params, args.amortize, args.learnable_reverse, args.vanilla_vae_epoches,\n",
    "                                                                                                args.hoffman_idea, args.separate_params, args.use_batchnorm, args.decoder))\n",
    "        torch.save(target.decoder,\n",
    "            './models/{}/best_decoder_data_{}_K_{}_N_{}_fixtransition_{}_amortize_{}_learnreverse_{}_vanillaepoches_{}_hoffmanlike_{}_separateparams_{}_usebatchnorm_{}_decoder_{}.pt'.format(args.data,\n",
    "                                    args.data, args.K, args.N, args.fix_transition_params, args.amortize, args.learnable_reverse, args.vanilla_vae_epoches,\n",
    "                                                                                                args.hoffman_idea, args.separate_params, args.use_batchnorm, args.decoder))\n",
    "        torch.save(transitions,\n",
    "            './models/{}/best_transitions_data_{}_K_{}_N_{}_fixtransition_{}_amortize_{}_learnreverse_{}_vanillaepoches_{}_hoffmanlike_{}_separateparams_{}_usebatchnorm_{}_decoder_{}.pt'.format(args.data,\n",
    "                                    args.data, args.K, args.N, args.fix_transition_params, args.amortize, args.learnable_reverse, args.vanilla_vae_epoches,\n",
    "                                                                                                args.hoffman_idea, args.separate_params, args.use_batchnorm, args.decoder))\n",
    "        if args.learnable_reverse:\n",
    "            torch.save(reverse_kernel,\n",
    "            './models/{}/best_reverse_data_{}_K_{}_N_{}_fixtransition_{}_amortize_{}_learnreverse_{}_vanillaepoches_{}_hoffmanlike_{}_separateparams_{}_usebatchnorm_{}_decoder_{}.pt'.format(args.data,\n",
    "                                    args.data, args.K, args.N, args.fix_transition_params, args.amortize, args.learnable_reverse, args.vanilla_vae_epoches,\n",
    "                                                                                                args.hoffman_idea, args.separate_params, args.use_batchnorm, args.decoder))\n",
    "\n",
    "    else:\n",
    "        current_tolerance += 1\n",
    "        if current_tolerance >= args.early_stopping_tolerance:\n",
    "            print(\"Early stopping on epoch {} (effectively trained for {} epoches)\".format(ep,\n",
    "                                              ep - args.early_stopping_tolerance))\n",
    "            break\n",
    "                \n",
    "    if ep % print_info_ == 0:\n",
    "        target.decoder.eval()\n",
    "        print('Current epoch:', (ep + 1), '\\t', 'Current ELBO train:', elbo_full.detach().mean().item())\n",
    "        print('Best elbo validation', best_elbo)\n",
    "        print('Current elbo validation', current_elbo_val)\n",
    "        plot_digit_samples(samples=get_samples(target.decoder, random_code), args=args, epoch=ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_elbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo_full.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(args.learnable_reverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(args, device=args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.data == 'toy_data':\n",
    "    encoder = Inf_network_simple(kwargs=args).to(args.device)\n",
    "    target = NN_Gaussian(kwargs=args, model=Gen_network_simple(args.z_dim, args), device=args.device).to(args.device)   \n",
    "else:\n",
    "    encoder = Inf_network(kwargs=args).to(args.device)\n",
    "    target = NN_bernoulli(kwargs=args, model=Gen_network(args.z_dim, args), device=args.device).to(args.device)\n",
    "\n",
    "params = [encoder.parameters(), target.parameters()]\n",
    "optimizer = torch.optim.Adam(params=itertools.chain(*params), lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_info_ = 10\n",
    "\n",
    "# with torch.autograd.detect_anomaly():\n",
    "for ep in tqdm(range(args.num_epoches)): # cycle over epoches\n",
    "    for b_num, batch_train in enumerate(dataset.next_train_batch()): # cycle over batches\n",
    "        plt.close()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        mu, sigma = encoder(batch_train) # sample mu and sigma from encoder\n",
    "        u = args.std_normal.sample(mu.shape) # sample random tensor for reparametrization trick\n",
    "        z = mu + sigma * u # reperametrization trick\n",
    "        \n",
    "        mu_dec, sigma_dec = target.decoder(z)\n",
    "        if args.data == 'toy_data':\n",
    "            log_numenator = torch.distributions.Normal(loc=mu_dec, scale=sigma_dec).log_prob(batch_train).sum(1) + args.std_normal.log_prob(z).sum(1)\n",
    "        else:\n",
    "            log_numenator = torch.distributions.Bernoulli(logits=mu_dec).log_prob(batch_train).sum([1, 2, 3]) + args.std_normal.log_prob(z).sum(1)\n",
    "        log_denumenator = -torch.sum(torch.log(sigma), 1) + args.std_normal.log_prob(u).sum(1)\n",
    "        elbo = torch.mean(log_numenator - log_denumenator)\n",
    "        (-elbo).backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    if ep % print_info_ == 0:\n",
    "        if args.data == 'toy_data':\n",
    "            print('Current epoch:', (ep + 1), '\\t', 'Current ELBO:', elbo.detach().mean().item())\n",
    "            print('Mean abs mu0:', torch.mean(torch.abs(mu_dec[:, 0])).cpu().detach().numpy())\n",
    "            print('Mean sigma', torch.mean(sigma).cpu().detach().numpy())\n",
    "            print('Max sigma', torch.max(sigma).cpu().detach().numpy())\n",
    "            print('Min sigma', torch.min(sigma).cpu().detach().numpy())\n",
    "            plt.scatter(batch_train.cpu().detach().numpy()[:, 0], batch_train.cpu().detach().numpy()[:, 1], label='Data')\n",
    "            plt.scatter(mu_dec.cpu().detach().numpy()[:, 0], mu_dec.cpu().detach().numpy()[:, 1], label='Reconstructed')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        else:\n",
    "            plot_digit_samples(samples=get_samples(target.decoder, random_code), args=args, epoch=ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.decoder.linear1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Condatorch",
   "language": "python",
   "name": "condatorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
