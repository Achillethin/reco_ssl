{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "from kernels import HMC_our, HMC_vanilla, Reverse_kernel_sampling\n",
    "from target import GMM_target, GMM_target2, Funnel, Gaussian_target, Banana\n",
    "from args import get_args\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = GMM_target(args)\n",
    "args.data_name = target.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_samples = target.get_samples(n=1000).cpu().numpy()\n",
    "plt.scatter(target_samples[:, 0], target_samples[:, 1])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we fix K\n",
    "args.n_steps = args.K # how many samples to use\n",
    "args.n_warmup = 0 # num of first samples to refuse\n",
    "\n",
    "args.n_chains = 10 # how many chains to model\n",
    "args.data_dim = 2\n",
    "args.z_dim = 2\n",
    "args.train_batch_size = 500\n",
    "# args.early_stopping_tolerance = 1000\n",
    "\n",
    "limit = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_kernel = HMC_vanilla(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla HMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_new = args.std_normal.sample((args.n_chains, args.data_dim))\n",
    "p_new = args.std_normal.sample((args.n_chains, args.data_dim))\n",
    "        \n",
    "print(\"Now we are running warmup!\")\n",
    "iterator = tqdm(range(args.n_warmup))\n",
    "for i in iterator:\n",
    "    q_new, p_new, _, _, a, _ = vanilla_kernel.make_transition(q_old=q_new, p_old=p_new, target_distr=target)\n",
    "iterator.close()\n",
    "    \n",
    "samples_vanilla = torch.empty((args.n_steps, args.n_chains, args.data_dim), device=args.device, dtype=args.torchType)\n",
    "\n",
    "print(\"Now we are sampling!\")\n",
    "iterator = tqdm(range(args.n_steps))\n",
    "for i in iterator:\n",
    "    q_new, p_new, _, _, a, _ = vanilla_kernel.make_transition(q_old=q_new, p_old=p_new, target_distr=target)\n",
    "    samples_vanilla[i] = q_new\n",
    "iterator.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=args.n_chains // 5, ncols=5, figsize=(18, 7), dpi=300)\n",
    "plt.suptitle('HMC vanilla')\n",
    "for chain_num in range(args.n_chains):\n",
    "    ax[chain_num // 5, chain_num % 5].scatter(target_samples[:, 0], target_samples[:, 1], c='r', label='Target')\n",
    "    ax[chain_num // 5, chain_num % 5].scatter(samples_vanilla[:, chain_num, 0].cpu(), samples_vanilla[:, chain_num, 1].cpu(), label='Result')\n",
    "    ax[chain_num // 5, chain_num % 5].set_xlim(-limit, limit)\n",
    "    ax[chain_num // 5, chain_num % 5].set_ylim(-limit, limit)\n",
    "    ax[chain_num // 5, chain_num % 5].set_aspect('equal')\n",
    "    ax[chain_num // 5, chain_num % 5].legend()\n",
    "plt.tight_layout();\n",
    "plt.subplots_adjust(left=0., right=1., bottom=0., top=0.9, wspace=0.1, hspace=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_new = args.std_normal.sample((target_samples.shape[0], args.z_dim))\n",
    "p_new = args.std_normal.sample((target_samples.shape[0], args.z_dim))\n",
    "\n",
    "iterator = tqdm(range(args.n_steps))\n",
    "for i in iterator:\n",
    "    q_new, p_new, _, _, a, _ = vanilla_kernel.make_transition(q_old=q_new, p_old=p_new, target_distr=target)\n",
    "iterator.close()\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(target_samples[:, 0], target_samples[:, 1], label='Target')\n",
    "plt.scatter(q_new.cpu().detach().numpy()[:, 0], q_new.cpu().detach().numpy()[:, 1], label='HMC')\n",
    "plt.legend()\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hoffman HMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Minimize KL first\n",
    "\n",
    "mu_init_hoff = nn.Parameter(torch.zeros(args.data_dim, device=args.device, dtype=args.torchType))\n",
    "sigma_init_hoff = nn.Parameter(torch.ones(args.data_dim, device=args.device, dtype=args.torchType))\n",
    "optimizer = torch.optim.Adam(params=[mu_init_hoff, sigma_init_hoff], lr=args.learning_rate)\n",
    "\n",
    "init_distr = torch.distributions.Normal(loc=mu_init_hoff, scale=nn.functional.softplus(sigma_init_hoff))\n",
    "for i in tqdm(range(20000)):\n",
    "#     pdb.set_trace()\n",
    "    u_init =  args.std_normal.sample((500,args.data_dim ))\n",
    "    q_init = mu_init_hoff + nn.functional.softplus(sigma_init_hoff) * u_init\n",
    "\n",
    "    current_kl = args.std_normal.log_prob(u_init).sum(1) - torch.sum(nn.functional.softplus(sigma_init_hoff).log()) - target.get_logdensity(z=q_init)\n",
    "    torch.mean(current_kl).backward() ## minimize KL\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if i % 2000 == 0:\n",
    "        print(current_kl.mean().cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot new prior\n",
    "prior_old = args.std_normal.sample((1000, 2)).cpu().detach().numpy()\n",
    "\n",
    "mu_init_hoff.requires_grad_(False)\n",
    "sigma_init_hoff.requires_grad_(False)\n",
    "prior_new = mu_init_hoff + args.std_normal.sample((1000, 2)) * nn.functional.softplus(sigma_init_hoff)\n",
    "prior_new = prior_new.cpu().detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(13, 5), sharex=True, sharey=True)\n",
    "ax[0].scatter(prior_old[:, 0], prior_old[:, 1])\n",
    "ax[1].scatter(prior_new[:, 0], prior_new[:, 1])\n",
    "ax[0].set_title('Old prior')\n",
    "ax[1].set_title('New prior');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Then run HMC (without warmup) of length K\n",
    "\n",
    "q_new = mu_init_hoff + args.std_normal.sample((args.n_chains, args.z_dim)) * nn.functional.softplus(sigma_init_hoff)\n",
    "p_new = args.std_normal.sample((args.n_chains, args.z_dim))\n",
    "        \n",
    "print(\"Now we are running warmup!\")\n",
    "iterator = tqdm(range(args.n_warmup))\n",
    "for i in iterator:\n",
    "    q_new, p_new, _, _, a, _ = vanilla_kernel.make_transition(q_old=q_new, p_old=p_new, target_distr=target)\n",
    "iterator.close()\n",
    "    \n",
    "samples_hoffman = torch.empty((args.n_steps, args.n_chains, args.z_dim), device=args.device, dtype=args.torchType)\n",
    "\n",
    "print(\"Now we are sampling!\")\n",
    "iterator = tqdm(range(args.n_steps))\n",
    "for i in iterator:\n",
    "    q_new, p_new, _, _, a, _ = vanilla_kernel.make_transition(q_old=q_new, p_old=p_new, target_distr=target)\n",
    "    samples_hoffman[i] = q_new\n",
    "iterator.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot received chains\n",
    "\n",
    "limit = 15\n",
    "fig, ax = plt.subplots(nrows=args.n_chains // 5, ncols=5, figsize=(18, 7), dpi=300)\n",
    "plt.suptitle('HMC Hoffman')\n",
    "for chain_num in range(args.n_chains):\n",
    "    ax[chain_num // 5, chain_num % 5].scatter(target_samples[:, 0], target_samples[:, 1], c='r', label='Target')\n",
    "    ax[chain_num // 5, chain_num % 5].scatter(samples_hoffman[:, chain_num, 0].cpu(), samples_hoffman[:, chain_num, 1].cpu(), label='Result')\n",
    "    ax[chain_num // 5, chain_num % 5].set_xlim(-limit, limit)\n",
    "    ax[chain_num // 5, chain_num % 5].set_ylim(-limit, limit)\n",
    "    ax[chain_num // 5, chain_num % 5].set_aspect('equal')\n",
    "    ax[chain_num // 5, chain_num % 5].legend()\n",
    "plt.tight_layout();\n",
    "plt.subplots_adjust(left=0., right=1., bottom=0., top=0.9, wspace=0.1, hspace=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_new = mu_init_hoff + args.std_normal.sample((target_samples.shape[0], args.z_dim)) * nn.functional.softplus(sigma_init_hoff)\n",
    "p_new = args.std_normal.sample((target_samples.shape[0], args.z_dim))\n",
    "\n",
    "iterator = tqdm(range(args.n_steps))\n",
    "for i in iterator:\n",
    "    q_new, p_new, _, _, a, _ = vanilla_kernel.make_transition(q_old=q_new, p_old=p_new, target_distr=target)\n",
    "iterator.close()\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(target_samples[:, 0], target_samples[:, 1], label='Target')\n",
    "plt.scatter(q_new.cpu().detach().numpy()[:, 0], q_new.cpu().detach().numpy()[:, 1], label='HMC Hoffman')\n",
    "plt.legend()\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neutra HMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Minimize KL first\n",
    "\n",
    "from pyro.nn import AutoRegressiveNN, DenseNN\n",
    "from pyro.distributions.transforms import NeuralAutoregressive, AffineAutoregressive, AffineCoupling\n",
    "\n",
    "num_flows = 10\n",
    "flow_type = 'IAF' # 'RNVP'\n",
    "\n",
    "flow_list = []\n",
    "for i in range(num_flows):\n",
    "    if flow_type == 'IAF':\n",
    "        one_arn = AutoRegressiveNN(args.data_dim, [2 * args.data_dim]).to(args.device)\n",
    "        one_flow = AffineAutoregressive(one_arn)\n",
    "    elif flow_type == 'RNVP':\n",
    "        hypernet = DenseNN(input_dim=args.data_dim // 2, hidden_dims=[2 * args.data_dim, 2 * args.data_dim],\n",
    "                param_dims=[args.data_dim - args.z_dim // 2, args.data_dim - args.data_dim // 2]).to(args.device)\n",
    "        one_flow = AffineCoupling(args.data_dim // 2, hypernet).to(args.device)\n",
    "    flow_list.append(one_flow)\n",
    "flows = nn.ModuleList(flow_list)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=flows.parameters(), lr=args.learning_rate)\n",
    "\n",
    "for i in tqdm(range(20000)):\n",
    "#     pdb.set_trace()\n",
    "    q_new = args.std_normal.sample((100, args.data_dim))\n",
    "    u = q_new\n",
    "    q_prev = q_new\n",
    "    sum_log_jacobian = 0.\n",
    "    for j in range(len(flows)):\n",
    "        q_new = flows[j](q_prev)\n",
    "        sum_log_jacobian += flows[j].log_abs_det_jacobian(q_prev, q_new)\n",
    "        q_prev = q_new\n",
    "        \n",
    "    current_kl = args.std_normal.log_prob(u).sum(1) - sum_log_jacobian - target.get_logdensity(z=q_new)\n",
    "    torch.mean(current_kl).backward() ## minimize KL\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if i % 2000 == 0:\n",
    "        print(current_kl.mean().cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot new prior\n",
    "prior = args.std_normal.sample((1000, 2))\n",
    "\n",
    "for p in flows.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "prior_pushforward = prior\n",
    "for i in range(len(flows)):\n",
    "    prior_pushforward = flows[i](prior_pushforward)\n",
    "prior_pushforward = prior_pushforward.cpu().detach().numpy()\n",
    "prior = prior.cpu().detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(13, 5), sharex=True, sharey=True)\n",
    "ax[0].scatter(prior[:, 0], prior[:, 1])\n",
    "ax[1].scatter(prior_pushforward[:, 0], prior_pushforward[:, 1])\n",
    "ax[0].set_title('Prior')\n",
    "ax[1].set_title('Prior pushforward');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Then run HMC (without warmup) of length K in warped space\n",
    "\n",
    "q_new = args.std_normal.sample((args.n_chains, args.z_dim))\n",
    "p_new = args.std_normal.sample((args.n_chains, args.z_dim))\n",
    "        \n",
    "print(\"Now we are running warmup!\")\n",
    "iterator = tqdm(range(args.n_warmup))\n",
    "for i in iterator:\n",
    "    q_new, p_new, _, _, a, _ = vanilla_kernel.make_transition(q_old=q_new, p_old=p_new, target_distr=target, flows=flows)\n",
    "iterator.close()\n",
    "\n",
    "samples_neutrahmc = torch.empty((args.n_steps, args.n_chains, args.z_dim), device=args.device, dtype=args.torchType)\n",
    "\n",
    "print(\"Now we are sampling!\")\n",
    "iterator = tqdm(range(args.n_steps))\n",
    "for i in iterator:\n",
    "#     pdb.set_trace()\n",
    "    q_new, p_new, _, _, a, _ = vanilla_kernel.make_transition(q_old=q_new, p_old=p_new, target_distr=target, flows=flows)\n",
    "    ### Pushforward obtained samples in warped space to original space\n",
    "    q_tr = q_new.detach()\n",
    "    for j in range(len(flows)):\n",
    "        q_tr = flows[j](q_tr)\n",
    "    samples_neutrahmc[i] = q_tr\n",
    "iterator.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot received chains\n",
    "\n",
    "limit = 15\n",
    "fig, ax = plt.subplots(nrows=args.n_chains // 5, ncols=5, figsize=(18, 7), dpi=300)\n",
    "plt.suptitle('Neutra HMC')\n",
    "for chain_num in range(args.n_chains):\n",
    "    ax[chain_num // 5, chain_num % 5].scatter(target_samples[:, 0], target_samples[:, 1], c='r', label='Target')\n",
    "    ax[chain_num // 5, chain_num % 5].scatter(samples_neutrahmc[:, chain_num, 0].cpu(), samples_neutrahmc[:, chain_num, 1].cpu(), label='Result')\n",
    "    ax[chain_num // 5, chain_num % 5].set_xlim(-limit, limit)\n",
    "    ax[chain_num // 5, chain_num % 5].set_ylim(-limit, limit)\n",
    "    ax[chain_num // 5, chain_num % 5].set_aspect('equal')\n",
    "    ax[chain_num // 5, chain_num % 5].legend()\n",
    "plt.tight_layout();\n",
    "plt.subplots_adjust(left=0., right=1., bottom=0., top=0.9, wspace=0.1, hspace=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_new = args.std_normal.sample((target_samples.shape[0], args.z_dim))\n",
    "p_new = args.std_normal.sample((target_samples.shape[0], args.z_dim))\n",
    "\n",
    "iterator = tqdm(range(args.n_steps))\n",
    "for i in iterator:\n",
    "    q_new, p_new, _, _, a, _ = vanilla_kernel.make_transition(q_old=q_new, p_old=p_new, target_distr=target, flows=flows)\n",
    "iterator.close()\n",
    "q_new = q_new.detach()\n",
    "for j in range(len(flows)):\n",
    "    q_new = flows[j](q_new)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(target_samples[:, 0], target_samples[:, 1], label='Target')\n",
    "plt.scatter(q_new.cpu().detach().numpy()[:, 0], q_new.cpu().detach().numpy()[:, 1], label='Neutra HMC')\n",
    "plt.legend()\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ours HMC (w/o reparametrization trick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.learnable_reverse:\n",
    "    reverse_kernel = Reverse_kernel_sampling(kwargs=args).to(args.device)\n",
    "    reverse_params = reverse_kernel.parameters()\n",
    "else:\n",
    "    reverse_params = list([])\n",
    "\n",
    "if args.amortize:\n",
    "    transitions = HMC_our(kwargs=args).to(args.device)\n",
    "    our_kernel_best_wo = HMC_our(kwargs=args).to(args.device)\n",
    "else:\n",
    "    transitions = nn.ModuleList([HMC_our(kwargs=args).to(args.device) for _ in range(args['K'])])\n",
    "    our_kernel_best_wo = nn.ModuleList([HMC_our(kwargs=args).to(args.device) for _ in range(args.K)])\n",
    "        \n",
    "if args.fix_transition_params:\n",
    "    for p in transitions.parameters():\n",
    "        transitions.requires_grad_(False)\n",
    "    \n",
    "mu_init_wo = nn.Parameter(torch.zeros(args.data_dim, device=args.device, dtype=args.torchType))\n",
    "sigma_init_wo = nn.Parameter(torch.ones(args.data_dim, device=args.device, dtype=args.torchType))\n",
    "\n",
    "params = list(transitions.parameters()) + list(reverse_params) + [mu_init_wo, sigma_init_wo]\n",
    "\n",
    "optimizer = torch.optim.Adam(params=params, lr=args.learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=200, factor=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.autograd.detect_anomaly():\n",
    "print_info_ = 1000\n",
    "torch_log_2 = torch.tensor(np.log(2.), device=args.device, dtype=args.torchType)\n",
    "best_elbo = -float(\"inf\")\n",
    "\n",
    "x_lim = limit\n",
    "y_lim = limit\n",
    "\n",
    "def compute_loss(q_new, p_new, q_old, p_old, sum_log_alpha, sum_log_jac, all_directions=None, mu=None, sigma=None, u=None):\n",
    "    if args.learnable_reverse:\n",
    "        log_r = reverse_kernel(z_fin=q_new, a=all_directions)\n",
    "    else:\n",
    "        log_r = -args.K * torch_log_2\n",
    "    log_p = target.get_logdensity(z=q_new) + args.std_normal.log_prob(p_new).sum(1)\n",
    "\n",
    "    sum_log_sigma = torch.sum(nn.functional.softplus(sigma).log())\n",
    "\n",
    "    log_m = -sum_log_sigma + args.std_normal.log_prob(u).sum(1) + args.std_normal.log_prob(p_old).sum(1) - sum_log_jac + sum_log_alpha\n",
    "\n",
    "    elbo_full = log_p + log_r - log_m\n",
    "    grad_elbo = torch.mean(log_p + log_r + (sum_log_alpha - sum_log_sigma - 0.5 * torch.sum((q_old - mu)**2 / sigma**2, 1)) * (elbo_full.detach() - 1.))\n",
    "    return elbo_full.detach().mean().item(), grad_elbo\n",
    "\n",
    "\n",
    "iterator = tqdm(range(args.num_batches))\n",
    "cur_tolerance = 0\n",
    "for batch_num in iterator:\n",
    "    plt.close()\n",
    "    cond_vectors = [args.std_normal.sample((args.train_batch_size, args.z_dim)) for k in range(args.K)]\n",
    "\n",
    "    # sample initial q and p\n",
    "    u = args.std_normal.sample((args.train_batch_size, args.z_dim))\n",
    "    q_old = mu_init_wo + u * nn.functional.softplus(sigma_init_wo)\n",
    "    p_old = args.std_normal.sample((args.train_batch_size, args.z_dim))\n",
    "\n",
    "    # prepare tensors for auxilary stuff\n",
    "    sum_log_alpha = torch.zeros(q_old.shape[0], dtype=args.torchType, device=args.device) # for grad log alpha accumulation\n",
    "    sum_log_jacobian = torch.zeros(q_old.shape[0], dtype=args.torchType, device=args.device) # for log_jacobian accumulation\n",
    "    \n",
    "    q_old = q_old.detach()\n",
    "    p_old = p_old.detach()\n",
    "    q = q_old\n",
    "    p = p_old\n",
    "    if (batch_num) % print_info_ == 0:\n",
    "        array_z = []\n",
    "        array_directions = []\n",
    "        array_alpha = []\n",
    "        array_prop = []\n",
    "\n",
    "    if args.learnable_reverse:\n",
    "        all_directions = torch.tensor([], device=args.device)\n",
    "    else:\n",
    "        all_directions = None\n",
    "\n",
    "    # run training cycle\n",
    "    for k in range(args.K):\n",
    "        if args.amortize:\n",
    "            q, p, log_jac, current_log_alphas, directions, q_prop = transitions.make_transition(q_old=q,\n",
    "                                            p_old=p, k=cond_vectors[k], target_distr=target)\n",
    "        else:\n",
    "            q, p, log_jac, current_log_alphas, directions, q_prop = transitions[k].make_transition(q_old=q,\n",
    "                                                        p_old=p, k=cond_vectors[k], target_distr=target) # sample a_i -- directions\n",
    "#         if (batch_num) % print_info_ == 0:\n",
    "#             print('On batch number {} and on k = {} we have for 0: {} and for +1: {}'.format(batch_num, k + 1,\n",
    "#                                                         (directions==0.).to(float).mean(), (directions==1.).to(float).mean()))\n",
    "#             print('Step size for {}-th transition is {}'.format(k, transitions[k].gamma.detach().exp()))\n",
    "#             print('Alpha (ref) for {}-th transition is {}'.format(k, torch.sigmoid(transitions[k].alpha_logit.detach())))\n",
    "        q = q.detach()\n",
    "        p = p.detach()\n",
    "        sum_log_alpha = sum_log_alpha + current_log_alphas\n",
    "        sum_log_jacobian = sum_log_jacobian + log_jac\n",
    "        if (batch_num) % print_info_ == 0:\n",
    "            array_z.append(q.detach())\n",
    "            array_directions.append(directions.detach())\n",
    "            array_alpha.append(current_log_alphas.detach())\n",
    "            array_prop.append(q_prop.detach())\n",
    "        if args.learnable_reverse:\n",
    "            all_directions = torch.cat([all_directions, directions.detach().view(-1, 1)], dim=1)\n",
    "    elbo_full, grad_elbo = compute_loss(q_new=q, p_new=p, q_old=q_old, p_old=p_old,\n",
    "                                        sum_log_jac=sum_log_jacobian,\n",
    "                                            sum_log_alpha=sum_log_alpha, all_directions=all_directions, mu=mu_init_wo,\n",
    "                                       sigma=sigma_init_wo, u=u)\n",
    "    if (batch_num ) % print_info_ == 0:\n",
    "        plt.scatter(q.cpu().detach()[:, 0], q.cpu().detach()[:, 1])\n",
    "        plt.show();\n",
    "        for param_group in optimizer.param_groups:\n",
    "            current_lr = param_group['lr']\n",
    "        print('Current lr: ', current_lr)\n",
    "        print('On batch number {} ELBO is {}'.format(batch_num, elbo_full))\n",
    "        print('On batch number {} Best ELBO is {}'.format(batch_num, best_elbo))\n",
    "\n",
    "#         fig, ax = plt.subplots(ncols=args.K, figsize=(30, 10))\n",
    "#         label = ['Same', 'Forward']\n",
    "#         for kk in range(args.K):\n",
    "#             ax[kk].scatter(array_prop[kk][:, 0].cpu().numpy(), array_prop[kk][:, 1].cpu().numpy(), color='r', label='Proposals')\n",
    "#             for d in [0., 1.]:\n",
    "#                 z_c = array_z[kk][array_directions[kk]==d]\n",
    "#                 alpha_c = array_alpha[kk][array_directions[kk]==d].cpu().exp().numpy()\n",
    "#                 color = np.zeros((z_c.shape[0], 4))\n",
    "#                 color[:, 3] = alpha_c\n",
    "#                 color[:, int(d + 1)] = 1.\n",
    "#                 ax[kk].scatter(z_c[:, 0].cpu().numpy(), z_c[:, 1].cpu().numpy(), color=color, label=label[int(d)])\n",
    "# #                 ax[kk].set_xlim((-x_lim, x_lim))\n",
    "# #                 ax[kk].set_ylim((-y_lim, y_lim))\n",
    "#                 ax[kk].legend()\n",
    "        plt.show();\n",
    "    (-grad_elbo).backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    scheduler.step(elbo_full)\n",
    "\n",
    "    if np.isnan(elbo_full):\n",
    "        print('NAN appeared!')\n",
    "        iterator.close()\n",
    "        raise ValueError\n",
    "\n",
    "    if elbo_full > best_elbo:\n",
    "        best_elbo = elbo_full\n",
    "        our_kernel_best_wo.load_state_dict(transitions.state_dict())\n",
    "        cur_tolerance = 0\n",
    "    else:\n",
    "        cur_tolerance += 1\n",
    "        if cur_tolerance >= args.early_stopping_tolerance:\n",
    "            print('Early stopping on {}'.format(batch_num))\n",
    "            iterator.close()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our_kernel_best = transitions\n",
    "for p in our_kernel_best_wo.parameters():\n",
    "    p.requires_grad_(False)\n",
    "    \n",
    "mu_init_wo.requires_grad_(False)\n",
    "sigma_init_wo.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot new prior\n",
    "prior_old = args.std_normal.sample((1000, 2)).cpu().detach().numpy()\n",
    "\n",
    "prior_new = mu_init_wo + args.std_normal.sample((1000, 2)) * nn.functional.softplus(sigma_init_wo)\n",
    "prior_new = prior_new.cpu().detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(13, 5), sharex=True, sharey=True)\n",
    "ax[0].scatter(prior_old[:, 0], prior_old[:, 1])\n",
    "ax[1].scatter(prior_new[:, 0], prior_new[:, 1])\n",
    "ax[0].set_title('Old prior')\n",
    "ax[1].set_title('New prior');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_samples = 10000\n",
    "n_chains = 50\n",
    "repetitions = 1\n",
    "\n",
    "init_q = mu_init_wo + args.std_normal.sample((n_chains, args.z_dim)).requires_grad_(True) * nn.functional.softplus(sigma_init_wo)\n",
    "q = init_q\n",
    "our_samples = []\n",
    "# our_samples.append(q.cpu().detach().numpy())\n",
    "\n",
    "# init_p = args.std_normal.sample((n_samples, args.z_dim))\n",
    "# p = init_p\n",
    "\n",
    "for rep in range(repetitions):\n",
    "    init_p = args.std_normal.sample((n_chains, args.z_dim))\n",
    "    p = init_p\n",
    "    cond_vectors = [args.std_normal.sample(init_p.shape) for _ in range(args.K)]\n",
    "    for k in range(args.K):\n",
    "        if args.amortize:\n",
    "            q, p, _, _, _, _ = our_kernel_best_wo.make_transition(q_old=q,\n",
    "                                            p_old=p, k=cond_vectors[k], target_distr=target) # sample a_i -- directions\n",
    "        else:\n",
    "            q, p, _, _, _, _ = our_kernel_best_wo[k].make_transition(q_old=q,\n",
    "                                                        p_old=p, k=cond_vectors[k], target_distr=target) # sample a_i -- directions\n",
    "        our_samples.append(q.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_samples = np.array(our_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_ind = 11\n",
    "\n",
    "\n",
    "plt.scatter(target_samples[:, 0], target_samples[:, 1], label='Target')\n",
    "plt.scatter(our_samples[:, chain_ind, 0], our_samples[:, chain_ind, 1], label='our HMC')\n",
    "# plt.xlim((-4, 4))\n",
    "# plt.ylim((-4, 4))\n",
    "plt.axis('equal')\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_new = mu_init_wo + args.std_normal.sample((target_samples.shape[0], args.z_dim)) * nn.functional.softplus(sigma_init_wo)\n",
    "p_new = args.std_normal.sample((target_samples.shape[0], args.z_dim))\n",
    "\n",
    "iterator = tqdm(range(args.n_steps))\n",
    "\n",
    "for i in iterator:\n",
    "    cond_vectors = [args.std_normal.sample(p_new.shape) for k in range(args.K)]\n",
    "    if args.amortize:\n",
    "        q_new, p_new, _, _, a, _ = our_kernel_best_wo.make_transition(q_old=q_new, p_old=p_new, target_distr=target, k=cond_vectors[i])\n",
    "    else:\n",
    "        q_new, p_new, _, _, a, _ = our_kernel_best_wo[i].make_transition(q_old=q_new, p_old=p_new, target_distr=target, k=cond_vectors[i])\n",
    "iterator.close()\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(target_samples[:, 0], target_samples[:, 1], label='Target')\n",
    "plt.scatter(q_new.cpu().detach().numpy()[:, 0], q_new.cpu().detach().numpy()[:, 1], label='HMC ours w/o reparam')\n",
    "plt.legend()\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ours HMC (with reparametrization trick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.learnable_reverse:\n",
    "    reverse_kernel = Reverse_kernel_sampling(kwargs=args).to(args.device)\n",
    "    reverse_params = reverse_kernel.parameters()\n",
    "else:\n",
    "    reverse_params = list([])\n",
    "\n",
    "if args.amortize:\n",
    "    transitions = HMC_our(kwargs=args).to(args.device)\n",
    "    our_kernel_best_w = HMC_our(kwargs=args).to(args.device)\n",
    "else:\n",
    "    transitions = nn.ModuleList([HMC_our(kwargs=args).to(args.device) for _ in range(args['K'])])\n",
    "    our_kernel_best_w = nn.ModuleList([HMC_our(kwargs=args).to(args.device) for _ in range(args.K)])\n",
    "        \n",
    "if args.fix_transition_params:\n",
    "    for p in transitions.parameters():\n",
    "        transitions.requires_grad_(False)\n",
    "    \n",
    "mu_init_w = nn.Parameter(torch.zeros(args.data_dim, device=args.device, dtype=args.torchType))\n",
    "sigma_init_w = nn.Parameter(torch.ones(args.data_dim, device=args.device, dtype=args.torchType))\n",
    "\n",
    "params = list(transitions.parameters()) + list(reverse_params) + [mu_init_w, sigma_init_w]\n",
    "\n",
    "optimizer = torch.optim.Adam(params=params, lr=args.learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=200, factor=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.autograd.detect_anomaly():\n",
    "print_info_ = 1000\n",
    "torch_log_2 = torch.tensor(np.log(2.), device=args.device, dtype=args.torchType)\n",
    "best_elbo = -float(\"inf\")\n",
    "\n",
    "x_lim = limit\n",
    "y_lim = limit\n",
    "\n",
    "def compute_loss(q_new, p_new, u, p_old, sum_log_alpha, sum_log_jac, all_directions=None, sum_log_sigma=None):\n",
    "    if args.learnable_reverse:\n",
    "        log_r = reverse_kernel(z_fin=q_new.detach(), a=all_directions)\n",
    "    else:\n",
    "        log_r = -args.K * torch_log_2\n",
    "    log_p = target.get_logdensity(z=q_new) + args.std_normal.log_prob(p_new).sum(1)\n",
    "    log_m = -sum_log_sigma + args.std_normal.log_prob(u).sum(1) + args.std_normal.log_prob(p_old).sum(1) - sum_log_jac + sum_log_alpha\n",
    "    elbo_full = log_p + log_r - log_m\n",
    "    grad_elbo = torch.mean(elbo_full + elbo_full.detach() * sum_log_alpha)\n",
    "    return elbo_full.detach().mean().item(), grad_elbo\n",
    "\n",
    "\n",
    "iterator = tqdm(range(args.num_batches))\n",
    "cur_tolerance = 0\n",
    "for batch_num in iterator:\n",
    "    plt.close()\n",
    "    cond_vectors = [args.std_normal.sample((args.train_batch_size, args.z_dim)) for k in range(args.K)]\n",
    "\n",
    "    # sample initial q and p\n",
    "    u = args.std_normal.sample((args.train_batch_size, args.z_dim))\n",
    "    q_old = mu_init_w + u * nn.functional.softplus(sigma_init_w)\n",
    "    p_old = args.std_normal.sample((args.train_batch_size, args.z_dim))\n",
    "    sum_log_sigma = torch.sum(nn.functional.softplus(sigma_init_w).log())\n",
    "\n",
    "    # prepare tensors for auxilary stuff\n",
    "    sum_log_alpha = torch.zeros(q_old.shape[0], dtype=args.torchType, device=args.device) # for grad log alpha accumulation\n",
    "    sum_log_jacobian = torch.zeros(q_old.shape[0], dtype=args.torchType, device=args.device) # for log_jacobian accumulation\n",
    "\n",
    "    q = q_old\n",
    "    p = p_old\n",
    "    if (batch_num) % print_info_ == 0:\n",
    "        array_z = []\n",
    "        array_directions = []\n",
    "        array_alpha = []\n",
    "        array_prop = []\n",
    "\n",
    "    if args.learnable_reverse:\n",
    "        all_directions = torch.tensor([], device=args.device)\n",
    "    else:\n",
    "        all_directions = None\n",
    "        \n",
    "     # run training cycle\n",
    "    for k in range(args.K):\n",
    "        if args.amortize:\n",
    "            q, p, log_jac, current_log_alphas, directions, q_prop = transitions.make_transition(q_old=q,\n",
    "                                            p_old=p, k=cond_vectors[k], target_distr=target)\n",
    "        else:\n",
    "            q, p, log_jac, current_log_alphas, directions, q_prop = transitions[k].make_transition(q_old=q,\n",
    "                                                        p_old=p, k=cond_vectors[k], target_distr=target) # sample a_i -- directions\n",
    "#         if (batch_num) % print_info_ == 0:\n",
    "#             print('On batch number {} and on k = {} we have for 0: {} and for +1: {}'.format(batch_num, k + 1,\n",
    "#                                                         (directions==0.).to(float).mean(), (directions==1.).to(float).mean()))\n",
    "#             print('Step size for {}-th transition is {}'.format(k, transitions[k].gamma.detach().exp()))\n",
    "#             print('Alpha (ref) for {}-th transition is {}'.format(k, torch.sigmoid(transitions[k].alpha_logit.detach())))\n",
    "        sum_log_alpha = sum_log_alpha + current_log_alphas\n",
    "        sum_log_jacobian = sum_log_jacobian + log_jac\n",
    "        if (batch_num) % print_info_ == 0:\n",
    "            array_z.append(q.detach())\n",
    "            array_directions.append(directions.detach())\n",
    "            array_alpha.append(current_log_alphas.detach())\n",
    "            array_prop.append(q_prop.detach())\n",
    "        if args.learnable_reverse:\n",
    "            all_directions = torch.cat([all_directions, directions.detach().view(-1, 1)], dim=1)\n",
    "    elbo_full, grad_elbo = compute_loss(q_new=q, p_new=p, u=u, p_old=p_old, sum_log_jac=sum_log_jacobian,\n",
    "                                            sum_log_alpha=sum_log_alpha, all_directions=all_directions, sum_log_sigma=sum_log_sigma)\n",
    "    if (batch_num ) % print_info_ == 0:\n",
    "        plt.scatter(q.cpu().detach()[:, 0], q.cpu().detach()[:, 1])\n",
    "        plt.show();\n",
    "        for param_group in optimizer.param_groups:\n",
    "            current_lr = param_group['lr']\n",
    "        print('Current lr: ', current_lr)\n",
    "        print('On batch number {} ELBO is {}'.format(batch_num, elbo_full))\n",
    "        print('On batch number {} Best ELBO is {}'.format(batch_num, best_elbo))\n",
    "#         fig, ax = plt.subplots(ncols=args.K, figsize=(30, 10))\n",
    "#         label = ['Same', 'Forward']\n",
    "#         for kk in range(args.K):\n",
    "#             ax[kk].scatter(array_prop[kk][:, 0].cpu().numpy(), array_prop[kk][:, 1].cpu().numpy(), color='r', label='Proposals')\n",
    "#             for d in [0., 1.]:\n",
    "#                 z_c = array_z[kk][array_directions[kk]==d]\n",
    "#                 alpha_c = array_alpha[kk][array_directions[kk]==d].cpu().exp().numpy()\n",
    "#                 color = np.zeros((z_c.shape[0], 4))\n",
    "#                 color[:, 3] = alpha_c\n",
    "#                 color[:, int(d + 1)] = 1.\n",
    "#                 ax[kk].scatter(z_c[:, 0].cpu().numpy(), z_c[:, 1].cpu().numpy(), color=color, label=label[int(d)])\n",
    "# #                 ax[kk].set_xlim((-x_lim, x_lim))\n",
    "# #                 ax[kk].set_ylim((-y_lim, y_lim))\n",
    "#                 ax[kk].legend()\n",
    "        plt.show();\n",
    "    (-grad_elbo).backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    scheduler.step(elbo_full)\n",
    "\n",
    "    if np.isnan(elbo_full):\n",
    "        print('NAN appeared!')\n",
    "        iterator.close()\n",
    "        raise ValueError\n",
    "\n",
    "    if elbo_full > best_elbo:\n",
    "        best_elbo = elbo_full\n",
    "        our_kernel_best_w.load_state_dict(transitions.state_dict())\n",
    "        cur_tolerance = 0\n",
    "    else:\n",
    "        cur_tolerance += 1\n",
    "        if cur_tolerance >= args.early_stopping_tolerance:\n",
    "            print('Early stopping on {}'.format(batch_num))\n",
    "            iterator.close()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_new = mu_init_w + args.std_normal.sample((target_samples.shape[0], args.z_dim)) * nn.functional.softplus(sigma_init_w)\n",
    "p_new = args.std_normal.sample((target_samples.shape[0], args.z_dim))\n",
    "\n",
    "iterator = tqdm(range(args.n_steps))\n",
    "\n",
    "for i in iterator:\n",
    "    cond_vectors = [args.std_normal.sample(p_new.shape) for k in range(args.K)]\n",
    "    if args.amortize:\n",
    "        q_new, p_new, _, _, a, _ = our_kernel_best_w.make_transition(q_old=q_new, p_old=p_new, target_distr=target, k=cond_vectors[i])\n",
    "    else:\n",
    "        q_new, p_new, _, _, a, _ = our_kernel_best_w[i].make_transition(q_old=q_new, p_old=p_new, target_distr=target, k=cond_vectors[i])\n",
    "iterator.close()\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(target_samples[:, 0], target_samples[:, 1], label='Target')\n",
    "plt.scatter(q_new.cpu().detach().numpy()[:, 0], q_new.cpu().detach().numpy()[:, 1], label='HMC ours with reparam')\n",
    "plt.legend()\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot new prior\n",
    "prior_old = args.std_normal.sample((1000, 2)).cpu().detach().numpy()\n",
    "\n",
    "prior_new = mu_init_w + args.std_normal.sample((1000, 2)) * nn.functional.softplus(sigma_init_w)\n",
    "prior_new = prior_new.cpu().detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(13, 5), sharex=True, sharey=True)\n",
    "ax[0].scatter(prior_old[:, 0], prior_old[:, 1])\n",
    "ax[1].scatter(prior_new[:, 0], prior_new[:, 1])\n",
    "ax[0].set_title('Old prior')\n",
    "ax[1].set_title('New prior');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in our_kernel_best_w.parameters():\n",
    "    p.requires_grad_(False)\n",
    "    \n",
    "mu_init_w.requires_grad_(False)\n",
    "sigma_init_w.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(20, 15), dpi=300, sharex=True, sharey=True)\n",
    "\n",
    "target_samples = target.get_samples(n=100000).cpu().numpy()\n",
    "u = args.std_normal.sample((target_samples.shape[0], args.z_dim))\n",
    "\n",
    "##### Vanilla HMC\n",
    "\n",
    "q_new = u #args.std_normal.sample((target_samples.shape[0], args.z_dim))\n",
    "p_new = args.std_normal.sample((target_samples.shape[0], args.z_dim))\n",
    "\n",
    "iterator = tqdm(range(args.n_steps))\n",
    "for i in iterator:\n",
    "    q_new, p_new, _, _, a, _ = vanilla_kernel.make_transition(q_old=q_new, p_old=p_new, target_distr=target)\n",
    "iterator.close()\n",
    "\n",
    "samples_hmc = q_new.cpu().detach().numpy()\n",
    "\n",
    "###### Hoffman HMC\n",
    "\n",
    "q_new = mu_init_hoff + u * nn.functional.softplus(sigma_init_hoff)\n",
    "p_new = args.std_normal.sample((target_samples.shape[0], args.z_dim))\n",
    "\n",
    "iterator = tqdm(range(args.n_steps))\n",
    "for i in iterator:\n",
    "    q_new, p_new, _, _, a, _ = vanilla_kernel.make_transition(q_old=q_new, p_old=p_new, target_distr=target)\n",
    "iterator.close()\n",
    "\n",
    "samples_hoff = q_new.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "####### Neutra HMC\n",
    "\n",
    "q_new = u\n",
    "p_new = args.std_normal.sample((target_samples.shape[0], args.z_dim))\n",
    "\n",
    "iterator = tqdm(range(args.n_steps))\n",
    "for i in iterator:\n",
    "    q_new, p_new, _, _, a, _ = vanilla_kernel.make_transition(q_old=q_new, p_old=p_new, target_distr=target, flows=flows)\n",
    "iterator.close()\n",
    "\n",
    "q_new = q_new.detach()\n",
    "for j in range(len(flows)):\n",
    "    q_new = flows[j](q_new)\n",
    "\n",
    "samples_neutrahmc = q_new.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "###### Ours without reparam\n",
    "\n",
    "q_new = mu_init_wo + u * nn.functional.softplus(sigma_init_wo)\n",
    "p_new = args.std_normal.sample((target_samples.shape[0], args.z_dim))\n",
    "\n",
    "iterator = tqdm(range(args.n_steps))\n",
    "for i in iterator:\n",
    "    cond_vectors = [args.std_normal.sample(p_new.shape) for k in range(args.K)]\n",
    "    if args.amortize:\n",
    "        q_new, p_new, _, _, a, _ = our_kernel_best_wo.make_transition(q_old=q_new, p_old=p_new, target_distr=target, k=cond_vectors[i])\n",
    "    else:\n",
    "        q_new, p_new, _, _, a, _ = our_kernel_best_wo[i].make_transition(q_old=q_new, p_old=p_new, target_distr=target, k=cond_vectors[i])\n",
    "iterator.close()\n",
    "\n",
    "samples_wo = q_new.cpu().detach().numpy()\n",
    "\n",
    "###### Ours with reparam\n",
    "\n",
    "q_new = mu_init_w + u * nn.functional.softplus(sigma_init_w)\n",
    "p_new = args.std_normal.sample((target_samples.shape[0], args.z_dim))\n",
    "\n",
    "iterator = tqdm(range(args.n_steps))\n",
    "for i in iterator:\n",
    "    cond_vectors = [args.std_normal.sample(p_new.shape) for k in range(args.K)]\n",
    "    if args.amortize:\n",
    "        q_new, p_new, _, _, a, _ = our_kernel_best_w.make_transition(q_old=q_new, p_old=p_new, target_distr=target, k=cond_vectors[i])\n",
    "    else:\n",
    "        q_new, p_new, _, _, a, _ = our_kernel_best_w[i].make_transition(q_old=q_new, p_old=p_new, target_distr=target, k=cond_vectors[i])\n",
    "iterator.close()\n",
    "\n",
    "samples_w = q_new.cpu().detach().numpy()\n",
    "\n",
    "lim_x = 2\n",
    "lim_y = 2 * lim_x\n",
    "lim_y_min = -2\n",
    "alpha = 0.01\n",
    "\n",
    "##### Plotting\n",
    "plt.suptitle('K={}'.format(args.K))\n",
    "ax[0, 0].set_title('HMC')\n",
    "ax[0, 0].scatter(target_samples[:, 0], target_samples[:, 1], label='Target')\n",
    "ax[0, 0].scatter(samples_hmc[:, 0], samples_hmc[:, 1], label='HMC')\n",
    "prior_samples = u\n",
    "prior_samples = prior_samples.cpu().detach().numpy()\n",
    "ax[0, 0].scatter(prior_samples[:, 0], prior_samples[:, 1], label='Prior', alpha=alpha)\n",
    "# ax[0, 0].set_xlim((-x_lim, x_lim))\n",
    "# ax[0, 0].set_ylim((lim_y_min, y_lim))\n",
    "# ax[0, 0].set_aspect('equal', 'datalim')\n",
    "ax[0, 0].legend()\n",
    "\n",
    "ax[0, 1].set_title('HMC Hoffman')\n",
    "ax[0, 1].scatter(target_samples[:, 0], target_samples[:, 1], label='Target')\n",
    "ax[0, 1].scatter(samples_hoff[:, 0], samples_hoff[:, 1], label='Hoffman')\n",
    "prior_samples = mu_init_hoff + u * nn.functional.softplus(sigma_init_hoff)\n",
    "prior_samples = prior_samples.cpu().detach().numpy()\n",
    "ax[0, 1].scatter(prior_samples[:, 0], prior_samples[:, 1], label='Prior', alpha=alpha)\n",
    "# ax[0, 1].set_xlim((-x_lim, x_lim))\n",
    "# ax[0, 1].set_ylim((lim_y_min, y_lim))\n",
    "# ax[0, 1].set_aspect('equal', 'datalim')\n",
    "ax[0, 1].legend()\n",
    "\n",
    "ax[1, 0].set_title('Ours without reparam')\n",
    "ax[1, 0].scatter(target_samples[:, 0], target_samples[:, 1], label='Target')\n",
    "ax[1, 0].scatter(samples_wo[:, 0], samples_wo[:, 1], label='Without reparam')\n",
    "prior_samples = mu_init_wo + u * nn.functional.softplus(sigma_init_wo)\n",
    "prior_samples = prior_samples.cpu().detach().numpy()\n",
    "ax[1, 0].scatter(prior_samples[:, 0], prior_samples[:, 1], label='Prior', alpha=alpha)\n",
    "# ax[1, 0].set_xlim((-x_lim, x_lim))\n",
    "# ax[1, 0].set_ylim((lim_y_min, y_lim))\n",
    "# ax[1, 0].set_aspect('equal', 'datalim')\n",
    "ax[1, 0].legend()\n",
    "\n",
    "ax[1, 1].set_title('Ours with reparam')\n",
    "ax[1, 1].scatter(target_samples[:, 0], target_samples[:, 1], label='Target')\n",
    "ax[1, 1].scatter(samples_w[:, 0], samples_w[:, 1], label='With reparam')\n",
    "prior_samples = mu_init_w + u * nn.functional.softplus(sigma_init_w)\n",
    "prior_samples = prior_samples.cpu().detach().numpy()\n",
    "ax[1, 1].scatter(prior_samples[:, 0], prior_samples[:, 1], label='Prior', alpha=alpha)\n",
    "# ax[1, 1].set_xlim((-x_lim, x_lim))\n",
    "# ax[1, 1].set_ylim((lim_y_min, y_lim))\n",
    "# ax[1, 1].set_aspect('equal', 'datalim')\n",
    "ax[1, 1].legend();\n",
    "\n",
    "ax[2, 0].set_title('Neutra HMC')\n",
    "ax[2, 0].scatter(target_samples[:, 0], target_samples[:, 1], label='Target')\n",
    "ax[2, 0].scatter(samples_neutrahmc[:, 0], samples_neutrahmc[:, 1], label='Neutra HMC')\n",
    "prior_samples = u.cpu().detach().numpy()\n",
    "ax[2, 0].scatter(prior_samples[:, 0], prior_samples[:, 1], label='Prior', alpha=alpha)\n",
    "# ax[1, 1].set_xlim((-x_lim, x_lim))\n",
    "# ax[1, 1].set_ylim((lim_y_min, y_lim))\n",
    "# ax[1, 1].set_aspect('equal', 'datalim')\n",
    "ax[2, 0].legend();\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./pics_hmc/K_{}_N_{}_gamma_{}_alpha_{}_learnreverse_{}_fixparam_{}_amortize_{}_dataname_{}.png'.format(args.K, args.N, args.gamma, args.alpha,\n",
    "                                                      args.learnable_reverse, args.fix_transition_params, args.amortize, args.data_name), format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(10, 10), dpi=300)\n",
    "\n",
    "norm=mcolors.PowerNorm(0.4)\n",
    "bins = 1000\n",
    "lim_x = 15\n",
    "lim_y = 15\n",
    "lim_y_min = -15\n",
    "\n",
    "##### Plotting 2d Hists\n",
    "plt.suptitle('K={}'.format(args.K))\n",
    "\n",
    "ax[0, 0].set_title('HMC')\n",
    "h, _, _ = np.histogram2d(samples_hmc[:, 0], samples_hmc[:, 1],\n",
    "                             bins=bins, density=True, range=[[-lim_x, lim_x], [lim_y_min, lim_y]])\n",
    "ax[0, 0].axis('off')\n",
    "ax[0, 0].axis('equal')\n",
    "ax[0, 0].imshow(np.rot90(h), interpolation='lanczos', norm=norm)\n",
    "\n",
    "ax[0, 1].set_title('HMC Hoffman')\n",
    "h, _, _ = np.histogram2d(samples_hoff[:, 0], samples_hoff[:, 1],\n",
    "                             bins=bins, density=True, range=[[-lim_x, lim_x], [lim_y_min, lim_y]])\n",
    "ax[0, 1].axis('off')\n",
    "ax[0, 1].axis('equal')\n",
    "ax[0, 1].imshow(np.rot90(h), interpolation='lanczos', norm=norm)\n",
    "\n",
    "ax[1, 0].set_title('Ours without reparam')\n",
    "h, _, _ = np.histogram2d(samples_wo[:, 0], samples_wo[:, 1],\n",
    "                             bins=bins, density=True, range=[[-lim_x, lim_x], [lim_y_min, lim_y]])\n",
    "ax[1, 0].axis('off')\n",
    "ax[1, 0].axis('equal')\n",
    "ax[1, 0].imshow(np.rot90(h), interpolation='lanczos', norm=norm)\n",
    "\n",
    "ax[1, 1].set_title('Ours with reparam')\n",
    "h, _, _ = np.histogram2d(samples_w[:, 0], samples_w[:, 1],\n",
    "                             bins=bins, density=True, range=[[-lim_x, lim_x], [lim_y_min, lim_y]])\n",
    "ax[1, 1].axis('off')\n",
    "ax[1, 1].axis('equal')\n",
    "ax[1, 1].imshow(np.rot90(h), interpolation='lanczos', norm=norm)\n",
    "\n",
    "\n",
    "ax[2, 0].set_title('Neutra HMC')\n",
    "h, _, _ = np.histogram2d(samples_neutrahmc[:, 0], samples_neutrahmc[:, 1],\n",
    "                             bins=bins, density=True, range=[[-lim_x, lim_x], [lim_y_min, lim_y]])\n",
    "ax[2, 0].axis('off')\n",
    "ax[2, 0].axis('equal')\n",
    "ax[2, 0].imshow(np.rot90(h), interpolation='lanczos', norm=norm)\n",
    "\n",
    "ax[2, 1].axis('off')\n",
    "ax[2, 1].axis('equal')\n",
    "\n",
    "# plt.subplots_adjust(left=0.25, right=1., bottom=0., top=0.85, wspace=0.1, hspace=0.15)\n",
    "plt.tight_layout();\n",
    "plt.savefig('./pics_hmc/hist2d_K_{}_N_{}_gamma_{}_alpha_{}_learnreverse_{}_fixparam_{}_amortize_{}_dataname_{}.png'.format(args.K, args.N, args.gamma, args.alpha,\n",
    "                                                      args.learnable_reverse, args.fix_transition_params, args.amortize, args.data_name), format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     plt\n",
    "#     plt.axis('off')\n",
    "#     plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Condatorch",
   "language": "python",
   "name": "condatorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
