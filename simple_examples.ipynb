{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.insert(0, './src/')\n",
    "from target import NN_bernoulli, GMM_target2\n",
    "from kernels import HMC_our, Reverse_kernel\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Encoder' - simple matrix\n",
    "class Encoder_h(nn.Module):\n",
    "    def __init__(self, L, z_dim, device='cpu'):\n",
    "        super(Encoder_h, self).__init__()\n",
    "        self.L = L\n",
    "        self.z_dim = z_dim\n",
    "        self.mu = nn.Linear(in_features=self.L, out_features=self.z_dim, bias=False)\n",
    "        self.h = nn.Linear(in_features=self.L, out_features=self.z_dim)\n",
    "    def forward(self, x):\n",
    "        return self.mu(x), self.h(x)\n",
    "    \n",
    "# 'Encoder' - simple matrix\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, L, z_dim, device='cpu'):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.L = L\n",
    "        self.z_dim = z_dim\n",
    "        self.mu = nn.Linear(in_features=self.L, out_features=self.z_dim, bias=False)\n",
    "#         self.sigma = nn.Linear(in_features=self.L, out_features=self.z_dim, bias=False)\n",
    "    def forward(self, x):\n",
    "        return self.mu(x) #, F.softplus(self.sigma(x))\n",
    "    \n",
    "# 'Decoder' - simple matrix, return logits\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, L, z_dim, device='cpu'):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.L = L\n",
    "        self.z_dim = z_dim\n",
    "        self.W = nn.Linear(in_features=self.z_dim, out_features=self.L, bias=False)\n",
    "    def forward(self, z):\n",
    "        return self.W(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 2\n",
    "\n",
    "K = 1024\n",
    "z_dim = K\n",
    "N = 10000\n",
    "\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "args = dotdict({})\n",
    "args.K = 2\n",
    "args.N = 2\n",
    "args.z_dim = z_dim\n",
    "args.torchType = torch.float32\n",
    "args.device = device\n",
    "args.learnable_reverse = True\n",
    "args.learnable_accept = False\n",
    "args.num_epoches = 301\n",
    "args.train_batch_size = 200\n",
    "args.amortize = False\n",
    "args.gamma = 0.1 ## Stepsize\n",
    "args.alpha = 0.5  ## For partial momentum refresh\n",
    "args.train_only_inference_period = 10\n",
    "args.train_only_inference_cutoff = 5\n",
    "args.hoffman_idea = True\n",
    "args.separate_params = True\n",
    "args.use_barker = True\n",
    "\n",
    "\n",
    "std_normal = torch.distributions.Normal(loc=torch.tensor(0., device=device),\n",
    "                                                scale=torch.tensor(1., device=device))\n",
    "args.std_normal = std_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True decoder matrix\n",
      "tensor([[6.2430, 4.8961],\n",
      "        [4.6999, 3.8981],\n",
      "        [4.9468, 3.7630],\n",
      "        ...,\n",
      "        [6.0021, 6.3763],\n",
      "        [6.3833, 5.6087],\n",
      "        [5.6023, 6.4880]], device='cuda:1')\n",
      "---------------------------------------------------------------------------\n",
      "Generated data example:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [0., 0.],\n",
      "        [1., 1.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [1., 1.],\n",
      "        [0., 0.],\n",
      "        [1., 1.]], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "true_theta = std_normal.sample((z_dim, L)) + 5\n",
    "print('True decoder matrix')\n",
    "print(true_theta)\n",
    "print('-' * 75)\n",
    "data_probs = torch.sigmoid(std_normal.sample((N, z_dim)) @ true_theta)\n",
    "# data_probs = torch.sigmoid(torch.ones((N, z_dim), device=device) @ true_theta)\n",
    "data = torch.distributions.Bernoulli(probs=data_probs).sample()\n",
    "print('Generated data example:')\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMM with arbitraty many components\n",
    "comp_1 = 1\n",
    "comp_2 = 0.7\n",
    "args['num_gauss'] = 8\n",
    "args['p_gaussians'] = [torch.tensor(1. / args['num_gauss'], device=args.device, dtype=args.torchType)] * args['num_gauss']\n",
    "args['locs'] = [torch.tensor([0., comp_1], dtype=args.torchType, device=args.device),\n",
    "               torch.tensor([comp_2, comp_2], dtype=args.torchType, device=args.device),\n",
    "               torch.tensor([comp_1, 0.], dtype=args.torchType, device=args.device),\n",
    "               torch.tensor([comp_2, -comp_2], dtype=args.torchType, device=args.device),\n",
    "               torch.tensor([0., -comp_1], dtype=args.torchType, device=args.device),\n",
    "               torch.tensor([-comp_2, -comp_2], dtype=args.torchType, device=args.device),\n",
    "               torch.tensor([-comp_1, 0.], dtype=args.torchType, device=args.device),\n",
    "               torch.tensor([-comp_2, comp_2], dtype=args.torchType, device=args.device)]  # list of locations for each of these gaussians\n",
    "args['covs'] = [0.01 * torch.eye(2, dtype=args.torchType, device=args.device)] * args['num_gauss']   # list of covariance matrices for each of these\n",
    "\n",
    "target = GMM_target2(args, device)\n",
    "data = target.get_samples(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(data[:, 0].cpu(), data[:, 1].cpu())\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(data, batch_size=args.train_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/301 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-10-065764a77bb1>(53)<module>()\n",
      "-> mu, sigma = enc(batch_train) # sample mu and sigma from encoder\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  т\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NameError: name 'т' is not defined\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-10-065764a77bb1>(54)<module>()\n",
      "-> u = args.std_normal.sample(mu.shape) # sample random tensor for reparametrization trick\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-10-065764a77bb1>(55)<module>()\n",
      "-> z = mu + u * sigma # reperametrization trick\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-10-065764a77bb1>(57)<module>()\n",
      "-> mu_recovered = dec(z)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-10-065764a77bb1>(58)<module>()\n",
      "-> log_likelihood = torch.distributions.Normal(loc=mu_recovered, scale=torch.ones_like(mu_recovered)).log_prob(batch_train).sum(1)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-10-065764a77bb1>(61)<module>()\n",
      "-> sum_log_sigma = torch.sum(torch.log(sigma), 1)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-10-065764a77bb1>(62)<module>()\n",
      "-> log_prior = std_normal.log_prob(z).sum(1)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-10-065764a77bb1>(63)<module>()\n",
      "-> log_q = std_normal.log_prob((z - mu) / sigma).sum(1) - sum_log_sigma\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  std_normal.log_prob((z - mu) / sigma).sum(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1456.0608, -1448.1256, -1479.5078, -1463.3284, -1466.4100, -1483.7974,\n",
      "        -1466.6238, -1475.7617, -1436.3424, -1415.6838, -1457.5048, -1446.7180,\n",
      "        -1419.5261, -1470.8787, -1452.4762, -1442.6462, -1500.6689, -1451.3696,\n",
      "        -1453.7454, -1429.0546, -1466.9119, -1489.6396, -1443.4868, -1438.5718,\n",
      "        -1471.5824, -1463.5129, -1445.9814, -1448.9202, -1451.4958, -1473.0181,\n",
      "        -1475.3403, -1498.9502, -1455.5524, -1452.1858, -1451.4979, -1415.1685,\n",
      "        -1475.5250, -1448.7112, -1472.5867, -1461.3740, -1488.1918, -1425.4847,\n",
      "        -1480.3849, -1444.1392, -1463.0243, -1453.4033, -1449.3317, -1445.3717,\n",
      "        -1461.2603, -1453.1394, -1504.0110, -1453.7825, -1454.2452, -1438.1373,\n",
      "        -1481.6589, -1445.7634, -1467.9879, -1460.3843, -1466.9404, -1481.3132,\n",
      "        -1437.5222, -1463.0657, -1481.9489, -1464.8058, -1454.9524, -1463.6118,\n",
      "        -1450.4153, -1435.3107, -1448.3430, -1438.0537, -1468.8574, -1374.8412,\n",
      "        -1503.6826, -1481.2860, -1458.8955, -1438.3909, -1484.5153, -1443.4272,\n",
      "        -1478.0739, -1459.2008, -1437.9996, -1454.5834, -1413.7744, -1469.8787,\n",
      "        -1432.6293, -1442.9009, -1444.8232, -1455.7306, -1423.0466, -1451.9570,\n",
      "        -1430.5864, -1452.0454, -1425.6527, -1457.5156, -1473.3110, -1444.4377,\n",
      "        -1484.4749, -1459.5886, -1437.3479, -1462.6837, -1448.0560, -1437.7317,\n",
      "        -1445.6522, -1425.5807, -1429.7609, -1462.4839, -1463.0554, -1469.9758,\n",
      "        -1443.6926, -1488.2291, -1483.1322, -1463.6086, -1468.2919, -1466.0854,\n",
      "        -1453.8455, -1436.3472, -1417.9381, -1412.5964, -1438.9247, -1431.4125,\n",
      "        -1440.9994, -1462.0471, -1435.0676, -1493.2341, -1415.7065, -1441.2897,\n",
      "        -1471.7241, -1454.8882, -1434.5930, -1446.1340, -1426.5391, -1434.1321,\n",
      "        -1452.7899, -1459.3977, -1483.3561, -1466.2617, -1437.7714, -1487.5217,\n",
      "        -1511.8560, -1425.6808, -1409.5837, -1463.3774, -1446.3712, -1478.4092,\n",
      "        -1479.5798, -1451.1643, -1449.6479, -1445.6003, -1490.1787, -1408.5200,\n",
      "        -1457.4644, -1443.9341, -1443.0952, -1430.2365, -1466.4847, -1421.4360,\n",
      "        -1428.3149, -1484.7489, -1454.2921, -1421.4761, -1466.1763, -1480.1560,\n",
      "        -1473.7910, -1490.6021, -1429.9021, -1450.6973, -1435.7253, -1445.0776,\n",
      "        -1454.7393, -1464.9983, -1437.0358, -1480.6688, -1434.3372, -1419.0812,\n",
      "        -1490.6910, -1492.2828, -1407.7484, -1491.2705, -1448.0820, -1485.8015,\n",
      "        -1448.7158, -1462.8368, -1466.4041, -1465.1473, -1456.3279, -1453.3727,\n",
      "        -1447.1387, -1438.2954, -1489.8573, -1483.9415, -1430.0544, -1426.9790,\n",
      "        -1477.0615, -1452.9392, -1423.9982, -1422.6509, -1465.3278, -1492.8170,\n",
      "        -1419.0918, -1423.3237], device='cuda:1', grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  std_normal.log_prob((z - mu) / sigma).grad_fn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SubBackward0 object at 0x7fd32c07b290>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-10-065764a77bb1>(64)<module>()\n",
      "-> elbo = torch.mean(log_likelihood + log_prior - log_q)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-10-065764a77bb1>(70)<module>()\n",
      "-> (-elbo).backward()\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-10-065764a77bb1>(72)<module>()\n",
      "-> optimizer.step()\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-065764a77bb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0melbo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m#     scheduler.step(-elbo)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-065764a77bb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0melbo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m#     scheduler.step(-elbo)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/condatorch/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/condatorch/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 'Encoder' - simple matrix\n",
    "class Encoder_vae(nn.Module):\n",
    "    def __init__(self, L, z_dim):\n",
    "        super(Encoder_vae, self).__init__()\n",
    "        self.L = L\n",
    "        self.z_dim = z_dim\n",
    "        self.h1 = nn.Linear(in_features=self.L, out_features=K)\n",
    "        self.h2 = nn.Linear(in_features=K, out_features=K)\n",
    "        self.mu = nn.Linear(in_features=K, out_features=self.z_dim)\n",
    "        self.sigma = nn.Linear(in_features=K, out_features=self.z_dim)\n",
    "    def forward(self, x):\n",
    "        h1 = F.selu(self.h1(x))\n",
    "        h2 = F.selu(self.h2(h1))\n",
    "        return self.mu(h2), nn.functional.softplus(self.sigma(h2))\n",
    "    \n",
    "class Decoder_vae(nn.Module):\n",
    "    def __init__(self, L, z_dim):\n",
    "        super(Decoder_vae, self).__init__()\n",
    "        self.L = L\n",
    "        self.z_dim = z_dim\n",
    "        self.h1 = nn.Linear(in_features=self.z_dim, out_features=K)\n",
    "        self.h2 = nn.Linear(in_features=K, out_features=K)\n",
    "        self.mu = nn.Linear(in_features=K, out_features=self.L)\n",
    "        self.sigma = nn.Linear(in_features=K, out_features=self.L)\n",
    "    def forward(self, x):\n",
    "        h1 = F.selu(self.h1(x))\n",
    "        h2 = F.selu(self.h2(h1))\n",
    "        return self.mu(h2) #, nn.functional.softplus(self.sigma(h3))\n",
    "    \n",
    "enc = Encoder_vae(L=L, z_dim=z_dim).to(device)\n",
    "dec = Decoder_vae(L=L, z_dim=z_dim).to(device)\n",
    "\n",
    "# enc = Encoder(L=L, z_dim=z_dim).to(device)\n",
    "# dec = Decoder(L=L, z_dim=z_dim).to(device)\n",
    "\n",
    "std_normal = torch.distributions.Normal(loc=torch.tensor(0., device=device),\n",
    "                                                scale=torch.tensor(1., device=device))\n",
    "args.std_normal = std_normal\n",
    "\n",
    "params = list(enc.parameters()) + list(dec.parameters())\n",
    "optimizer = torch.optim.Adam(params=params, lr=1e-3)\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[500, 1000, 1500, 2000, 2500,\n",
    "#                                                                        3000, 3500, 4000, 4500], gamma=0.9) #torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0, last_epoch=-1)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, threshold=1e-2, eps=1e-6)\n",
    "\n",
    "\n",
    "print_info_ = 50\n",
    "# with torch.autograd.detect_anomaly():\n",
    "for ep in tqdm(range(args.num_epoches)): # cycle over epoches\n",
    "    plt.close()\n",
    "    for b_num, batch_train in enumerate(dataloader): # cycle over batches\n",
    "        pdb.set_trace()   \n",
    "        mu, sigma = enc(batch_train) # sample mu and sigma from encoder\n",
    "        u = args.std_normal.sample(mu.shape) # sample random tensor for reparametrization trick\n",
    "        z = mu + u * sigma # reperametrization trick\n",
    "        \n",
    "        mu_recovered = dec(z)\n",
    "        log_likelihood = torch.distributions.Normal(loc=mu_recovered, scale=torch.ones_like(mu_recovered)).log_prob(batch_train).sum(1)\n",
    "#         log_likelihood = torch.distributions.Bernoulli(probs=torch.sigmoid(mu_recovered)).log_prob(batch_train).sum(1)\n",
    "    \n",
    "        sum_log_sigma = torch.sum(torch.log(sigma), 1)\n",
    "        log_prior = std_normal.log_prob(z).sum(1)\n",
    "        log_q = std_normal.log_prob((z - mu) / sigma).sum(1) - sum_log_sigma\n",
    "        elbo = torch.mean(log_likelihood + log_prior - log_q)\n",
    "\n",
    "#         KLD = -0.5 * torch.mean(1 + 2 * sigma.log() - mu.pow(2) - sigma.pow(2))\n",
    "#         elbo = torch.mean(log_likelihood - KLD)\n",
    "        \n",
    "        \n",
    "        (-elbo).backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "#     scheduler.step(-elbo)\n",
    "    if ep % print_info_ == 0:\n",
    "        plt.scatter(batch_train.cpu().detach().numpy()[:, 0], batch_train.cpu().detach().numpy()[:, 1])\n",
    "        plt.scatter(mu_recovered.cpu().detach().numpy()[:, 0], mu_recovered.cpu().detach().numpy()[:, 1])\n",
    "        plt.show()\n",
    "        print('elbo:', elbo.cpu().detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dec.W.weight.T)\n",
    "# print(true_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = args.std_normal.sample((1000, args.z_dim)) # sample random tensor for reparametrization trick\n",
    "batch_recovered = dec(u).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(batch_recovered[:, 0], batch_recovered[:, 1])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[p for p in optimizer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.learnable_reverse:\n",
    "    enc = Encoder_h(L=L, z_dim=z_dim, device=device).to(device)\n",
    "else:\n",
    "    enc = Encoder(L=L, z_dim=z_dim, device=device).to(device)\n",
    "dec = Decoder(L=L, z_dim=z_dim, device=device).to(device)\n",
    "reverse_kernel = Reverse_kernel(args).to(device)\n",
    "target = NN_bernoulli({}, dec, device).to(device)\n",
    "transitions = nn.ModuleList([HMC_our(kwargs=args).to(args.device) for _ in range(args['K'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.learnable_reverse:\n",
    "    reverse_params = list(reverse_kernel.parameters())\n",
    "else:\n",
    "    reverse_params = list([])\n",
    "    \n",
    "if args.learnable_accept:\n",
    "    accept_func = Accept_func(kwargs=args).to(args.device)\n",
    "    accept_params = accept_func.parameters()\n",
    "else:\n",
    "    accept_params = list([])\n",
    "\n",
    "if args.separate_params:\n",
    "    params_inference = list(enc.parameters()) + list(reverse_kernel.parameters()) + list(transitions.parameters()) + accept_params\n",
    "    optimizer = torch.optim.Adam(params=target.parameters())\n",
    "    optimizer_inference = torch.optim.Adam(params=params_inference)\n",
    "else:\n",
    "    params = list(enc.parameters()) + list(target.parameters()) + list(transitions.parameters()) + reverse_params + accept_params\n",
    "    optimizer = torch.optim.Adam(params=params)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 200, 300, 400], gamma=0.5) #torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2717, -0.6099],\n",
       "        [-0.5089, -0.2191]], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.mu.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(z_new, p_new, u, p_old, x, sum_log_alpha, sum_log_jac, sum_log_sigma=0., mu=None, all_directions=None, h=None):\n",
    "    if args.learnable_reverse:\n",
    "        log_r = reverse_kernel(z_fin=z_new.detach(), h=h.detach(), a=all_directions)\n",
    "        log_m = args.std_normal.log_prob(u).sum(1) + args.std_normal.log_prob(p_old).sum(1) - sum_log_jac - sum_log_sigma + sum_log_alpha\n",
    "    else:\n",
    "        log_r = -args.K * torch.tensor(np.log(2.), device=device)\n",
    "        log_m = args.std_normal.log_prob(u).sum(1) + args.std_normal.log_prob(p_old).sum(1) - sum_log_jac - sum_log_sigma + sum_log_alpha\n",
    "        \n",
    "    log_p = target.get_logdensity(z=z_new, x=x, args=args) + args.std_normal.log_prob(p_new.sum(1))\n",
    "    elbo_full = log_p + log_r - log_m\n",
    "    grad_elbo = torch.mean(elbo_full + elbo_full.detach() * sum_log_alpha)\n",
    "    return elbo_full, grad_elbo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/5000 [00:00<01:22, 60.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On batch number 1/2 and on k = 1 we have for  0: 0.56 and for +1: 0.44\n",
      "On batch number 1/2 and on k = 2 we have for  0: 0.48 and for +1: 0.52\n",
      "obj_1: tensor(1.3110, grad_fn=<MeanBackward0>)\n",
      "obj_2: tensor(-7.4289, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 512/5000 [00:07<01:03, 70.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On batch number 1/2 and on k = 1 we have for  0: 0.32 and for +1: 0.68\n",
      "On batch number 1/2 and on k = 2 we have for  0: 0.56 and for +1: 0.44\n",
      "obj_1: tensor(0.4931, grad_fn=<MeanBackward0>)\n",
      "obj_2: tensor(-7.1133, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1007/5000 [00:14<00:56, 70.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On batch number 1/2 and on k = 1 we have for  0: 0.48 and for +1: 0.52\n",
      "On batch number 1/2 and on k = 2 we have for  0: 0.52 and for +1: 0.48\n",
      "obj_1: tensor(0.3981, grad_fn=<MeanBackward0>)\n",
      "obj_2: tensor(-7.4428, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 1513/5000 [00:21<00:49, 70.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On batch number 1/2 and on k = 1 we have for  0: 0.52 and for +1: 0.48\n",
      "On batch number 1/2 and on k = 2 we have for  0: 0.54 and for +1: 0.46\n",
      "obj_1: tensor(0.2315, grad_fn=<MeanBackward0>)\n",
      "obj_2: tensor(-6.9668, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2012/5000 [00:28<00:39, 74.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On batch number 1/2 and on k = 1 we have for  0: 0.5 and for +1: 0.5\n",
      "On batch number 1/2 and on k = 2 we have for  0: 0.54 and for +1: 0.46\n",
      "obj_1: tensor(0.2556, grad_fn=<MeanBackward0>)\n",
      "obj_2: tensor(-6.9195, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2508/5000 [00:34<00:33, 75.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On batch number 1/2 and on k = 1 we have for  0: 0.64 and for +1: 0.36\n",
      "On batch number 1/2 and on k = 2 we have for  0: 0.42 and for +1: 0.58\n",
      "obj_1: tensor(0.2367, grad_fn=<MeanBackward0>)\n",
      "obj_2: tensor(-7.2249, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3012/5000 [00:41<00:26, 75.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On batch number 1/2 and on k = 1 we have for  0: 0.5 and for +1: 0.5\n",
      "On batch number 1/2 and on k = 2 we have for  0: 0.6 and for +1: 0.4\n",
      "obj_1: tensor(0.2640, grad_fn=<MeanBackward0>)\n",
      "obj_2: tensor(-6.9882, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 3508/5000 [00:48<00:19, 75.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On batch number 1/2 and on k = 1 we have for  0: 0.5 and for +1: 0.5\n",
      "On batch number 1/2 and on k = 2 we have for  0: 0.52 and for +1: 0.48\n",
      "obj_1: tensor(0.2268, grad_fn=<MeanBackward0>)\n",
      "obj_2: tensor(-6.8993, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4012/5000 [00:55<00:13, 74.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On batch number 1/2 and on k = 1 we have for  0: 0.56 and for +1: 0.44\n",
      "On batch number 1/2 and on k = 2 we have for  0: 0.42 and for +1: 0.58\n",
      "obj_1: tensor(0.1106, grad_fn=<MeanBackward0>)\n",
      "obj_2: tensor(-6.8914, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 4508/5000 [01:01<00:06, 70.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On batch number 1/2 and on k = 1 we have for  0: 0.56 and for +1: 0.44\n",
      "On batch number 1/2 and on k = 2 we have for  0: 0.42 and for +1: 0.58\n",
      "obj_1: tensor(0.1705, grad_fn=<MeanBackward0>)\n",
      "obj_2: tensor(-6.8818, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:08<00:00, 72.80it/s]\n"
     ]
    }
   ],
   "source": [
    "print_info_ = 500\n",
    "\n",
    "for ep in tqdm(range(args.num_epoches)): # cycle over epoches\n",
    "    for b_num, batch_train in enumerate(dataloader): # cycle over batches\n",
    "        if args.learnable_reverse:\n",
    "            mu, h = enc(batch_train) # sample mu and sigma from encoder\n",
    "        else:\n",
    "            mu = enc(batch_train) # sample mu and sigma from encoder\n",
    "            h = None\n",
    "        u = args.std_normal.sample(mu.shape) # sample random tensor for reparametrization trick\n",
    "        z = mu + u # reperametrization trick\n",
    "\n",
    "        p_old = args.std_normal.sample(z.shape)\n",
    "        cond_vectors = [args.std_normal.sample(p_old.shape) for _ in range(args.K)]\n",
    "\n",
    "        sum_log_alpha = torch.zeros(mu.shape[0], dtype=args.torchType, device=args.device) # for grad log alpha accumulation\n",
    "        sum_log_jacobian = torch.zeros(mu.shape[0], dtype=args.torchType, device=args.device) # for log_jacobian accumulation\n",
    "        p = p_old\n",
    "        if args.learnable_reverse:\n",
    "            all_directions = torch.tensor([], device=args.device)\n",
    "        else:\n",
    "            all_directions = None\n",
    "        for k in range(args.K):\n",
    "            # sample alpha - transition probabilities \n",
    "            if args.amortize:\n",
    "#                     pdb.set_trace()\n",
    "                z, p, log_jac, current_log_alphas, directions, _ = transitions.make_transition(q_old=z, x=batch_train,\n",
    "                                                    p_old=p, k=cond_vectors[k], target_distr=target, args=args)\n",
    "            else:\n",
    "                z, p, log_jac, current_log_alphas, directions, _ = transitions[k].make_transition(q_old=z, x=batch_train,\n",
    "                                                                    p_old=p, k=cond_vectors[k], target_distr=target, args=args) # sample a_i -- directions\n",
    "            if ep  % print_info_ == 0 and b_num % (100 * print_info_) == 0:\n",
    "                print('On batch number {}/{} and on k = {} we have for  0: {} and for +1: {}'.format(b_num + 1,\n",
    "                                                                        data.shape[0] // args['train_batch_size'],\n",
    "                                                                           k + 1,\n",
    "                                                    (directions==0.).to(float).mean(),\n",
    "                                                                    (directions==1.).to(float).mean()))\n",
    "                if args.amortize:\n",
    "                    print('Stepsize {}'.format(np.exp(transitions.gamma.cpu().detach().item())))\n",
    "                    print('Autoregression coeff {}'.format(torch.sigmoid(transitions.alpha_logit).cpu().detach().item()))\n",
    "            if args.learnable_reverse:\n",
    "                all_directions = torch.cat([all_directions, directions.view(-1, 1)], dim=1)\n",
    "            # Accumulate alphas\n",
    "            sum_log_alpha = sum_log_alpha + current_log_alphas\n",
    "            sum_log_jacobian = sum_log_jacobian + log_jac  # refresh log jacobian\n",
    "        ##############################################\n",
    "        if args.hoffman_idea:\n",
    "            if args.learnable_reverse:\n",
    "                log_r = reverse_kernel(z_fin=z.detach(), h=h.detach(), a=all_directions)\n",
    "                log_m = args.std_normal.log_prob(u).sum(1) + args.std_normal.log_prob(p_old).sum(1) - sum_log_jacobian + sum_log_alpha\n",
    "            else:\n",
    "                log_r = -args.K * torch_log_2\n",
    "                log_m = args.std_normal.log_prob(u).sum(1) + args.std_normal.log_prob(p_old).sum(1) - sum_log_jacobian + sum_log_alpha\n",
    "            log_p = target.get_logdensity(z=z, x=batch_train, args=args) + args.std_normal.log_prob(p.sum(1))\n",
    "            elbo_full = log_p + log_r - log_m\n",
    "    #                 pdb.set_trace()\n",
    "            ### Gradient of the first objective:\n",
    "#             target.eval()\n",
    "            obj_1 = torch.mean(elbo_full + elbo_full.detach() * sum_log_alpha)\n",
    "            (-obj_1).backward(retain_graph=True)\n",
    "            optimizer_inference.step()\n",
    "            optimizer_inference.zero_grad()\n",
    "            optimizer.zero_grad() \n",
    "\n",
    "            ### Gradient of the second objective:\n",
    "#             target.train()\n",
    "            log_p = target.get_logdensity(z=z.detach(), x=batch_train, args=args) + args.std_normal.log_prob(p.detach()).sum(1)\n",
    "            obj_2 = torch.mean(log_p)\n",
    "            (-obj_2).backward()\n",
    "            optimizer.step()\n",
    "            optimizer_inference.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "        else:\n",
    "            elbo_full, grad_elbo = compute_loss(z_new=z, p_new=p, u=u, p_old=p_old, x=batch_train, sum_log_alpha=sum_log_alpha,\n",
    "                                                sum_log_jac=sum_log_jacobian, mu=mu,\n",
    "                                                all_directions=all_directions, h=h)\n",
    "            (-grad_elbo).backward()\n",
    "\n",
    "                \n",
    "        if args.separate_params: # if we separate params of inference part and generation part\n",
    "            optimizer_inference.step() # we always perform step for inference part\n",
    "            if ep % args.train_only_inference_period > args.train_only_inference_cutoff: # but sometimes for gen\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            optimizer_inference.zero_grad()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "            \n",
    "        if ep  % print_info_ == 0 and b_num % (100 * print_info_) == 0:\n",
    "            if args.hoffman_idea:\n",
    "                print('obj_1:', obj_1)\n",
    "                print('obj_2:', obj_2)\n",
    "            else:\n",
    "                print('elbo:', elbo_full.mean().cpu().detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0561,  0.0124],\n",
       "        [-0.4709,  0.4122]], grad_fn=<PermuteBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.decoder.W.weight.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.decoder.W.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9196,  1.2161],\n",
      "        [ 1.1737, -0.9498]])\n"
     ]
    }
   ],
   "source": [
    "print(true_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0286,  0.0149],\n",
       "        [-0.4285,  0.3998]], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.mu.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0039, -0.0011],\n",
       "        [-0.0083, -0.0068]], grad_fn=<PermuteBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec.d.weight.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.d.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6969, 0.2562],\n",
      "        [0.7179, 0.3912]])\n"
     ]
    }
   ],
   "source": [
    "print(true_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b_num, batch_train in enumerate(dataloader): # cycle over batches\n",
    "    mu = enc(batch_train) # sample mu and sigma from encoder\n",
    "    u = args.std_normal.sample(mu.shape) # sample random tensor for reparametrization trick\n",
    "    z = mu + u # reperametrization trick\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.distributions.Bernoulli(probs=torch.sigmoid(dec(z[:10]))).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Condatorch",
   "language": "python",
   "name": "condatorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
