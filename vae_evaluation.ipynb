{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, './src')\n",
    "\n",
    "from kernels import HMC_our, HMC_vanilla, Reverse_kernel\n",
    "from models import Gen_network, Inf_network\n",
    "from target import NN_bernoulli\n",
    "from utils import plot_digit_samples, get_samples\n",
    "from args import get_args\n",
    "\n",
    "from data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_name = 'data_mnist_K_2_N_1_fixtransition_False_amortize_False_learnreverse_True'\n",
    "\n",
    "NUM_EVALS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args()\n",
    "\n",
    "args.device = \"cuda:1\"\n",
    "args.std_normal = torch.distributions.Normal(loc=torch.tensor(0., dtype=args.torchType, device=args.device),\n",
    "                                            scale=torch.tensor(1., dtype=args.torchType, device=args.device))\n",
    "dataset = Dataset(args, args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### First, we load train models and fix their params\n",
    "decoder = torch.load('./models/mnist/best_decoder_{}.pt'.format(general_name), map_location=args.device)\n",
    "for p in decoder.parameters():\n",
    "    p.requires_grad_(False)\n",
    "target = NN_bernoulli(args, decoder, args.device)\n",
    "    \n",
    "encoder = torch.load('./models/mnist/best_encoder_{}.pt'.format(general_name), map_location=args.device)\n",
    "for p in encoder.parameters():\n",
    "    p.requires_grad_(False)\n",
    "    \n",
    "transitions = torch.load('./models/mnist/best_transitions_{}.pt'.format(general_name), map_location=args.device)\n",
    "if args.amortize:\n",
    "    transitions.device = args.device\n",
    "else:\n",
    "    for k in range(args.K):\n",
    "        transitions[k].device = args.device\n",
    "for p in transitions.parameters():\n",
    "    p.requires_grad_(False)\n",
    "    \n",
    "if args.learnable_reverse:\n",
    "    reverse_kernel = torch.load('./models/mnist/best_reverse_{}.pt'.format(general_name), map_location=args.device)\n",
    "    reverse_kernel.device = args.device\n",
    "    for p in reverse_kernel.parameters():\n",
    "        p.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLL estimation using importance sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(z_new, p_new, u, p_old, x, sum_log_alpha, sum_log_jac, sum_log_sigma, mu=None, all_directions=None):\n",
    "    if args.learnable_reverse:\n",
    "        log_r = reverse_kernel(z_fin=z_new.detach(), mu=mu.detach(), a=all_directions)\n",
    "        log_m = args.std_normal.log_prob(u).sum(1) + args.std_normal.log_prob(p_old).sum(1) - sum_log_jac - sum_log_sigma + sum_log_alpha\n",
    "    else:\n",
    "        log_r = 0 #-args.K * torch_log_2\n",
    "        log_m = args.std_normal.log_prob(u).sum(1) + args.std_normal.log_prob(p_old).sum(1) - sum_log_jac - sum_log_sigma # + sum_log_alpha\n",
    "    log_p = target.get_logdensity(z=z_new, x=x) + args.std_normal.log_prob(p_new).sum(1)\n",
    "    elbo_full = log_p + log_r - log_m\n",
    "    grad_elbo = torch.mean(elbo_full + elbo_full.detach() * sum_log_alpha)\n",
    "    return elbo_full, grad_elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_marginal_q(z_final, K, x, mu=None, sigma=None):\n",
    "#     aux_matrix = torch.empty((z_final.shape[0], 3**K), device=device, dtype=torchType)\n",
    "#     directions_all = np.array(list(itertools.product(np.arange(3) - 1, repeat=K)))\n",
    "#     cond_distr = torch.distributions.Normal(loc=mu, scale=sigma)\n",
    "#     for d_num, directions in enumerate(directions_all):\n",
    "#         sum_log_alpha = torch.zeros(z_final.shape[0], dtype=torchType, device=device) # for grad log alpha accumulation\n",
    "#         sum_log_jacobian = torch.zeros(z_final.shape[0], dtype=torchType, device=device) # for log_jacobian accumulation\n",
    "#         z = z_final\n",
    "#         for k in range(K)[::-1]:\n",
    "#             if directions[k]==1.:\n",
    "#                 current_log_alphas = transitions[k].get_log_alpha_1(z_old=z, x=x)\n",
    "#             elif directions[k]==-1:\n",
    "#                 current_log_alphas = transitions[k].get_log_alpha_m1(z_old=z, x=x)\n",
    "#             else:\n",
    "#                 current_log_alphas = transitions[k].get_log_alpha_0(z_old=z, x=x)\n",
    "#             z_upd, log_jac = transitions[k].make_transition(z_old=z, x=x, directions=directions[k] * torch.ones(z.shape[0], dtype=torchType, device=device),\n",
    "#                                                             k=cond_vectors[k]) # sample new positions and log_jacobians of transformations\n",
    "#             sum_log_jacobian = sum_log_jacobian + log_jac  # refresh log jacobian\n",
    "#             sum_log_alpha = sum_log_alpha + current_log_alphas\n",
    "#             z = z_upd\n",
    "#         log_q_joint = cond_distr.log_prob(z).sum(1) + sum_log_jacobian + sum_log_alpha\n",
    "#         aux_matrix[:, d_num] = log_q_joint\n",
    "#     loq_q_marginal = torch.logsumexp(aux_matrix, dim=1)\n",
    "#     return loq_q_marginal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 58/1000 [04:22<1:11:32,  4.56s/it]"
     ]
    }
   ],
   "source": [
    "means = []\n",
    "stds = []\n",
    "for _ in range(NUM_EVALS):\n",
    "    nll = []\n",
    "    for test_batch in tqdm(dataset.next_test_batch(), total=dataset.test.shape[0] // dataset.test_batch_size):\n",
    "        mu, sigma = encoder(test_batch)\n",
    "\n",
    "        sum_log_alpha = torch.zeros(mu.shape[0], dtype=args.torchType, device=args.device) # for grad log alpha accumulation\n",
    "        sum_log_jacobian = torch.zeros(mu.shape[0], dtype=args.torchType, device=args.device) # for log_jacobian accumulation\n",
    "        sum_log_sigma = torch.sum(torch.log(sigma), 1)\n",
    "\n",
    "        u = args.std_normal.sample(mu.shape)\n",
    "        z = mu + sigma * u\n",
    "\n",
    "        p_old = args.std_normal.sample(z.shape)\n",
    "        cond_vectors = [args.std_normal.sample(p_old.shape) for k in range(args.K)]\n",
    "        p = p_old\n",
    "        if args.learnable_reverse:\n",
    "            all_directions = torch.tensor([], device=args.device)\n",
    "        else:\n",
    "            all_directions = None\n",
    "\n",
    "        for k in range(args.K):\n",
    "    #         pdb.set_trace()\n",
    "            if args.amortize:\n",
    "                z, p, log_jac, current_log_alphas, directions, _ = transitions.make_transition(q_old=z, x=test_batch,\n",
    "                                                    p_old=p, k=cond_vectors[k], target_distr=target)\n",
    "            else:\n",
    "                z, p, log_jac, current_log_alphas, directions, _ = transitions[k].make_transition(q_old=z, x=test_batch,\n",
    "                                                                    p_old=p, k=cond_vectors[k], target_distr=target) # sample a_i -- directions\n",
    "            if args.learnable_reverse:\n",
    "                all_directions = torch.cat([all_directions, directions.view(-1, 1)], dim=1)\n",
    "            sum_log_alpha = sum_log_alpha + current_log_alphas\n",
    "            sum_log_jacobian = sum_log_jacobian + log_jac  # refresh log jacobian\n",
    "        nll_current, _ = compute_loss(z_new=z, p_new=p, u=u, p_old=p_old, x=test_batch, sum_log_alpha=sum_log_alpha,\n",
    "                                    sum_log_jac=sum_log_jacobian, sum_log_sigma=sum_log_sigma, mu=mu, all_directions=all_directions)\n",
    "\n",
    "        nll.append(nll_current.cpu().detach().numpy())\n",
    "    means.append(np.mean(nll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Condatorch",
   "language": "python",
   "name": "condatorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
